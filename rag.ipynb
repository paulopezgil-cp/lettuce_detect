{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7885d414",
   "metadata": {},
   "source": [
    "# 1. Description of the project\n",
    "\n",
    "In this project, a RAG system is implemented and used in combination with LettuceDetect.\n",
    "\n",
    "# 2. Setup\n",
    "\n",
    "1. **Install these packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0477ba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq langchain langchain-unstructured langchain-chroma langchain-openai unstructured langchain-community unstructured[pdf] dotenv lettucedetect gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4b2de",
   "metadata": {},
   "source": [
    "2. **Import the necessary modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "46796ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from lettucedetect.models.inference import HallucinationDetector\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af561e9",
   "metadata": {},
   "source": [
    "3. **Deploy an Azure OpenAI LLM resource and embedding resource**\n",
    "\n",
    "    Use the following link: https://ai.azure.com/\n",
    "4. **Save the details to the .env file:**\n",
    "    ```bash\n",
    "    echo AZURE_OPENAI_API_KEY=\\\"your-api-key-here\\\" >> .env\n",
    "    echo AZURE_OPENAI_API_VERSION=\\\"your-version-here\\\" >> .env\n",
    "    echo AZURE_OPENAI_ENDPOINT=\\\"your-endpoint-here\\\" >> .env\n",
    "    echo GPT_MODEL=\\\"your-llm-model-here\\\" >> .env\n",
    "    echo EMBEDDINGS_MODEL_NAME=\\\"your-embeddings-model-here\\\" >> .env\n",
    "    echo EMBEDDINGS_DEPLOYMENT=\\\"your-embeddings-deployment-here\\\" >> .env\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add108f6",
   "metadata": {},
   "source": [
    "# 3. ChromaDB setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2abe90",
   "metadata": {},
   "source": [
    "## 3.1 The text splitter\n",
    "\n",
    "The text splitter divides documents into manageable chunks to optimize downstream processing and retrieval in RAG workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5f04b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_splitter(data, debug = False):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "    )\n",
    "    if debug:\n",
    "        print(f\"Splitting {len(data)} documents into chunks...\")\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f83fd",
   "metadata": {},
   "source": [
    "## 3.2 The document loader\n",
    "\n",
    "The document loader reads and parses files from the corpus directory into structured document objects for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "235c37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(corpus_dir = \"./corpus/\", debug = False):\n",
    "    loaded_docs = []\n",
    "    if debug:\n",
    "        print(f\"Loading documents from {corpus_dir}...\")\n",
    "    for file in os.listdir(corpus_dir):\n",
    "        if debug:\n",
    "            print(f\"Loading {file}...\")\n",
    "        loader = UnstructuredLoader(corpus_dir + file, mode = 'single')\n",
    "        loaded_docs.extend(loader.load())\n",
    "    return loaded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8207d2a",
   "metadata": {},
   "source": [
    "## 3.3 The embedding client\n",
    "\n",
    "The embedding client initializes and manages Azure OpenAI embeddings for converting text into vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "821f66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(debug = False):\n",
    "    load_dotenv(find_dotenv())\n",
    "    model = os.getenv('EMBEDDINGS_MODEL_NAME')\n",
    "    api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    azure_deployment = os.getenv(\"EMBEDDINGS_DEPLOYMENT\")\n",
    "\n",
    "    # Validate required environment variables\n",
    "    if not all([model, api_key, api_version, azure_endpoint, azure_deployment]):\n",
    "        raise ValueError(\n",
    "            \"\"\"\n",
    "            Missing environment variables.\n",
    "            Please load all the required environment variables in the .env file:\n",
    "            EMBEDDINGS_MODEL_NAME, AZURE_OPENAI_API_KEY, AZURE_OPENAI_API_VERSION,\n",
    "            AZURE_OPENAI_ENDPOINT, EMBEDDINGS_DEPLOYMENT\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    # Initialize and return an Azure OpenAI embeddings client\n",
    "    if debug:\n",
    "        print(f\"Initializing embeddings with model: {model}, deployment: {azure_deployment}\")\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        model = model,\n",
    "        api_key = api_key,\n",
    "        api_version = api_version,\n",
    "        azure_endpoint = azure_endpoint,\n",
    "        azure_deployment = azure_deployment,\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efc443",
   "metadata": {},
   "source": [
    "## 3.4 The vector database\n",
    "\n",
    "The vector database stores document embeddings for fast similarity search and retrieval. Built with Chroma, it enables efficient access to relevant document chunks in RAG workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "87c2d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(document_list, database_dir = \"./chroma_db\", debug = False):\n",
    "    # Filter complex metadata from documents\n",
    "    if debug:\n",
    "        print(\"Filtering complex metadata from documents...\")\n",
    "    filtered_docs = filter_complex_metadata(document_list)\n",
    "\n",
    "    # Initialize the database from a given corpus of documents\n",
    "    embedding_model = embeddings(debug = debug)\n",
    "    if debug:\n",
    "        print(f\"Creating vector database with {len(filtered_docs)} documents...\")\n",
    "    vector_database = Chroma.from_documents(documents = filtered_docs,\n",
    "                                            embedding = embedding_model,\n",
    "                                            persist_directory = database_dir)\n",
    "\n",
    "    # Return the vector database instance\n",
    "    return Chroma(persist_directory = database_dir,\n",
    "                  embedding_function = embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b860d2e",
   "metadata": {},
   "source": [
    "## 3.5 The retriever\n",
    "\n",
    "The retriever fetches relevant document chunks from the vector database using embeddings to match user queries with semantically similar content for efficient retrieval in RAG workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "01c3e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(corpus_dir = \"./corpus/\", debug = False):\n",
    "    docs = load_documents(corpus_dir, debug = debug)\n",
    "    chunks = text_splitter(docs, debug = debug)\n",
    "    vectordb = create_database(chunks, debug = debug)\n",
    "    if debug:\n",
    "        print(\"Creating retriever from vector database...\")\n",
    "    retriever = vectordb.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96237913",
   "metadata": {},
   "source": [
    "## 3.6 Small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d5992ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from ./corpus/...\n",
      "Loading ford_f150_lightning_2024.txt...\n",
      "Loading audi_a4_2024.txt...\n",
      "Loading mercedes_cclass_2024.txt...\n",
      "Loading tesla_model_s_2024.txt...\n",
      "Loading toyota_camry_2024.txt...\n",
      "Loading bmw_3series_2024.txt...\n",
      "Loading jeep_wrangler_2024.txt...\n",
      "Loading porsche_911_carrera_2024.txt...\n",
      "Loading subaru_outback_2024.txt...\n",
      "Loading honda_accord_2024.txt...\n",
      "Splitting 271 documents into chunks...\n",
      "Filtering complex metadata from documents...\n",
      "Initializing embeddings with model: text-embedding-3-small, deployment: text-embedding-3-small\n",
      "Creating vector database with 271 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating retriever from vector database...\n"
     ]
    }
   ],
   "source": [
    "retriever = retriever(debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "da88a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='c18fe47b-c9ef-4e83-ab09-cd511c710c17', metadata={'parent_id': '0749961253165664e3d63731c5667eee', 'filename': 'honda_accord_2024.txt', 'file_directory': './corpus', 'filetype': 'text/plain', 'element_id': '56c4f36868129724874376f4e43bb081', 'source': './corpus/honda_accord_2024.txt', 'last_modified': '2025-07-24T15:17:12', 'category': 'NarrativeText'}, page_content='For those requiring additional power, the available 2.0-liter turbocharged four-cylinder engine generates an impressive 252 horsepower and 273 lb-ft of torque. This engine features a sophisticated cooling system and advanced turbocharger technology that delivers smooth, linear power delivery throughout the rev range.'),\n",
       " Document(id='d32faf07-3c50-4d6d-91ee-04493847cc5c', metadata={'last_modified': '2025-07-24T15:17:12', 'filetype': 'text/plain', 'source': './corpus/honda_accord_2024.txt', 'file_directory': './corpus', 'category': 'NarrativeText', 'parent_id': '0749961253165664e3d63731c5667eee', 'filename': 'honda_accord_2024.txt', 'element_id': '56c4f36868129724874376f4e43bb081'}, page_content='For those requiring additional power, the available 2.0-liter turbocharged four-cylinder engine generates an impressive 252 horsepower and 273 lb-ft of torque. This engine features a sophisticated cooling system and advanced turbocharger technology that delivers smooth, linear power delivery throughout the rev range.'),\n",
       " Document(id='7e350a05-f89c-4e46-a5c7-eac0f61245cc', metadata={'filename': 'honda_accord_2024.txt', 'file_directory': './corpus', 'parent_id': '0749961253165664e3d63731c5667eee', 'filetype': 'text/plain', 'category': 'NarrativeText', 'element_id': '56c4f36868129724874376f4e43bb081', 'last_modified': '2025-07-24T15:17:12', 'source': './corpus/honda_accord_2024.txt'}, page_content='For those requiring additional power, the available 2.0-liter turbocharged four-cylinder engine generates an impressive 252 horsepower and 273 lb-ft of torque. This engine features a sophisticated cooling system and advanced turbocharger technology that delivers smooth, linear power delivery throughout the rev range.'),\n",
       " Document(id='0a73b1cb-d91a-41f1-83c7-bdad3897ec40', metadata={'parent_id': '0749961253165664e3d63731c5667eee', 'filename': 'honda_accord_2024.txt', 'filetype': 'text/plain', 'source': './corpus/honda_accord_2024.txt', 'last_modified': '2025-07-24T15:17:12', 'element_id': '56c4f36868129724874376f4e43bb081', 'category': 'NarrativeText', 'file_directory': './corpus'}, page_content='For those requiring additional power, the available 2.0-liter turbocharged four-cylinder engine generates an impressive 252 horsepower and 273 lb-ft of torque. This engine features a sophisticated cooling system and advanced turbocharger technology that delivers smooth, linear power delivery throughout the rev range.')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"Which car features a turbocharged 3.0-liter inline-six engine\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c90de",
   "metadata": {},
   "source": [
    "# 4. The Hallucination detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aa8ac4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hallucination_from_context(context, question, answer, debug = False):\n",
    "    if debug:\n",
    "        print(f\"Predicting hallucination for question: {question}\")\n",
    "\n",
    "    # Initialize the hallucination detector with a transformer model\n",
    "    detector = HallucinationDetector(\n",
    "        method=\"transformer\",\n",
    "        model_path=\"KRLabsOrg/lettucedect-base-modernbert-en-v1\"\n",
    "    )\n",
    "\n",
    "    # Predict hallucination using the detector\n",
    "    result = detector.predict(context = context,\n",
    "                              question = question,\n",
    "                              answer = answer,\n",
    "                              output_format = \"spans\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "00515a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hallucination_from_corpus(corpus, question, answer, debug = False):\n",
    "    # Should take a list of strings as context\n",
    "    if debug:\n",
    "        print(f\"Detecting hallucination for question: {question} with answer: {answer}\")\n",
    "    \n",
    "    # Initialize the retriever with the corpus directory\n",
    "    retriever = retriever(corpus_dir = corpus, debug = debug)\n",
    "\n",
    "    # Retrieve relevant documents from the corpus\n",
    "    if debug:\n",
    "        print(f\"Retrieving relevant documents for question: {question}\")\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Predict hallucination using the predict_hallucination function\n",
    "    detected_hallucination = detect_hallucination_from_context(\n",
    "        context = retrieved_docs,\n",
    "        question = question,\n",
    "        answer = answer,\n",
    "        debug = debug\n",
    "    )\n",
    "\n",
    "    # Check if an hallucination was detected\n",
    "    hallucination_was_found = bool(detected_hallucination)\n",
    "\n",
    "    # Create an output string based on the result\n",
    "    hallucination_str = \"\"\n",
    "    for hallucination in detected_hallucination:\n",
    "        hallucination_str += f\"\"\"\n",
    "           '{hallucination['text']}' - Confidence = {hallucination['confidence']}\\n\n",
    "        \"\"\"\n",
    "\n",
    "    return hallucination_was_found, hallucination_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485658c4",
   "metadata": {},
   "source": [
    "# 5. The interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "69fcc583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/gradio/interface.py:425: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
      "  warnings.warn(\n",
      "INFO: HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: HEAD http://127.0.0.1:7861/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/gradio/queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 2235, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/gradio/utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_91388/3713416188.py\", line 7, in detect_hallucination_from_corpus\n",
      "    retriever = retriever(corpus_dir = corpus, debug = debug)\n",
      "                ^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'retriever' where it is not associated with a value\n"
     ]
    }
   ],
   "source": [
    "rag_application = gr.Interface(\n",
    "    fn = detect_hallucination_from_corpus,\n",
    "    allow_flagging = \"never\",\n",
    "    inputs = [\n",
    "        # Drag and drop files, returns a list of file paths\n",
    "        gr.UploadButton(label = \"Upload PDF/txt files\",\n",
    "                        file_count = 'multiple',\n",
    "                        file_types = ['.pdf', '.txt'],\n",
    "                        type = \"filepath\"),\n",
    "        gr.Textbox(label = \"Prompt\",\n",
    "                   placeholder = \"Type your question here...\"),\n",
    "        gr.Textbox(label = \"Answer\",\n",
    "                   lines = 3,\n",
    "                   placeholder = \"type the answer here...\"),\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Textbox(label = \"Status\"),\n",
    "        gr.Textbox(label = \"Detected Hallucinations\",\n",
    "                   lines = 5)\n",
    "    ],\n",
    "    title = \"RAG system with Hallucination Detection\",\n",
    "    description = \"\"\"\n",
    "        Upload a PDF document and ask any question.\n",
    "        The chatbot will try to answer using the provided document.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "rag_application.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
