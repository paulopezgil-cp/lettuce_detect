{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7885d414",
   "metadata": {},
   "source": [
    "# 1. Description of the project\n",
    "\n",
    "In this project, a RAG system is implemented and used in combination with LettuceDetect.\n",
    "\n",
    "# 2. Setup\n",
    "\n",
    "1. **Install these packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qq langchain langchain-unstructured langchain-chroma langchain-openai unstructured langchain-community unstructured[pdf] dotenv lettucedetect gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4b2de",
   "metadata": {},
   "source": [
    "2. **Import the necessary modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46796ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from lettucedetect.models.inference import HallucinationDetector\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af561e9",
   "metadata": {},
   "source": [
    "3. **Deploy an Azure OpenAI LLM resource and embedding resource**\n",
    "\n",
    "    Use the following link: https://ai.azure.com/\n",
    "4. **Save the details to the .env file:**\n",
    "    ```bash\n",
    "    echo AZURE_OPENAI_API_KEY=\\\"your-api-key-here\\\" >> .env\n",
    "    echo AZURE_OPENAI_API_VERSION=\\\"your-version-here\\\" >> .env\n",
    "    echo AZURE_OPENAI_ENDPOINT=\\\"your-endpoint-here\\\" >> .env\n",
    "    echo GPT_MODEL=\\\"your-llm-model-here\\\" >> .env\n",
    "    echo EMBEDDINGS_MODEL_NAME=\\\"your-embeddings-model-here\\\" >> .env\n",
    "    echo EMBEDDINGS_DEPLOYMENT=\\\"your-embeddings-deployment-here\\\" >> .env\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add108f6",
   "metadata": {},
   "source": [
    "# 3. ChromaDB setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2abe90",
   "metadata": {},
   "source": [
    "## 3.1 The text splitter\n",
    "\n",
    "The text splitter divides documents into manageable chunks to optimize downstream processing and retrieval in RAG workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_splitter(data, debug = False):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "    )\n",
    "    if debug:\n",
    "        print(f\"Splitting {len(data)} documents into chunks...\")\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f83fd",
   "metadata": {},
   "source": [
    "## 3.2 The document loader\n",
    "\n",
    "The document loader reads and parses files from the corpus directory into structured document objects for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(corpus_dir = \"./corpus/\", debug = False):\n",
    "    # Load all documents from the corpus directory\n",
    "    loaded_docs = []\n",
    "    if debug:\n",
    "        print(f\"Loading documents from {corpus_dir}...\")\n",
    "    for file in os.listdir(corpus_dir):\n",
    "        if debug:\n",
    "            print(f\"Loading {file}...\")\n",
    "        loader = UnstructuredLoader(corpus_dir + file, mode = 'single')\n",
    "        loaded_docs.extend(loader.load())\n",
    "\n",
    "    # Filter complex metadata from loaded documents\n",
    "    if debug:\n",
    "        print(\"Filtering complex metadata...\")\n",
    "    filtered_docs = filter_complex_metadata(loaded_docs)\n",
    "\n",
    "    return filtered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8207d2a",
   "metadata": {},
   "source": [
    "## 3.3 The embedding client\n",
    "\n",
    "The embedding client initializes and manages Azure OpenAI embeddings for converting text into vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(debug = False):\n",
    "    load_dotenv(find_dotenv())\n",
    "    model = os.getenv('EMBEDDINGS_MODEL_NAME')\n",
    "    api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    azure_deployment = os.getenv(\"EMBEDDINGS_DEPLOYMENT\")\n",
    "\n",
    "    # Validate required environment variables\n",
    "    if not all([model, api_key, api_version, azure_endpoint, azure_deployment]):\n",
    "        raise ValueError(\n",
    "            \"\"\"\n",
    "            Missing environment variables.\n",
    "            Please load all the required environment variables in the .env file:\n",
    "            EMBEDDINGS_MODEL_NAME, AZURE_OPENAI_API_KEY, AZURE_OPENAI_API_VERSION,\n",
    "            AZURE_OPENAI_ENDPOINT, EMBEDDINGS_DEPLOYMENT\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    # Initialize and return an Azure OpenAI embeddings client\n",
    "    if debug:\n",
    "        print(f\"Initializing embeddings with model: {model}, deployment: {azure_deployment}\")\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        model = model,\n",
    "        api_key = api_key,\n",
    "        api_version = api_version,\n",
    "        azure_endpoint = azure_endpoint,\n",
    "        azure_deployment = azure_deployment,\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efc443",
   "metadata": {},
   "source": [
    "## 3.4 The vector database\n",
    "\n",
    "The vector database stores document embeddings for fast similarity search and retrieval. Built with Chroma, it enables efficient access to relevant document chunks in RAG workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c2d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(document_list, database_dir = \"./chroma_db\", debug = False):\n",
    "    # Initialize the database from a given corpus of documents\n",
    "    embedding_model = embeddings(debug = debug)\n",
    "\n",
    "    # Create the database if it doesn't exist\n",
    "    if not os.path.exists(database_dir) or not os.listdir(database_dir):\n",
    "        if debug:\n",
    "            print(f\"Creating vector database with {len(document_list)} documents...\")\n",
    "        return Chroma.from_documents(documents = document_list,\n",
    "                                                embedding = embedding_model,\n",
    "                                                persist_directory = database_dir)\n",
    "\n",
    "    # If the database exists, return it\n",
    "    return Chroma(persist_directory = database_dir,\n",
    "                  embedding_function = embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b860d2e",
   "metadata": {},
   "source": [
    "## 3.5 The retriever\n",
    "\n",
    "The retriever fetches relevant document chunks from the vector database using embeddings to match user queries with semantically similar content for efficient retrieval in RAG workflows. It is not based on LLMs, but purely on a similarity search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c3e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(corpus_dir = \"./corpus/\", debug = False):\n",
    "    docs = load_documents(corpus_dir, debug = debug)\n",
    "    chunks = text_splitter(docs, debug = debug)\n",
    "    vectordb = create_database(chunks, debug = debug)\n",
    "    if debug:\n",
    "        print(\"Creating retriever from vector database...\")\n",
    "    retriever = vectordb.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96237913",
   "metadata": {},
   "source": [
    "## 3.6 Small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5992ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_retriever = retriever(corpus_dir = \"./example_inputs/\", debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da88a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = test_retriever.get_relevant_documents(\"Which car features a turbocharged 3.0-liter inline-six engine\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c90de",
   "metadata": {},
   "source": [
    "# 4. The Hallucination detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ac4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hallucinations(context, question, answer, debug = False):\n",
    "    if debug:\n",
    "        print(f\"Predicting hallucination for question: {question}\")\n",
    "\n",
    "    # Initialize the hallucination detector with a transformer model\n",
    "    detector = HallucinationDetector(\n",
    "        method=\"transformer\",\n",
    "        model_path=\"KRLabsOrg/lettucedect-base-modernbert-en-v1\"\n",
    "    )\n",
    "\n",
    "    # Predict hallucination using the detector\n",
    "    result = detector.predict(context = context,\n",
    "                              question = question,\n",
    "                              answer = answer,\n",
    "                              output_format = \"spans\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485658c4",
   "metadata": {},
   "source": [
    "# 5. The interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00515a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_backend(uploaded_files, question, answer, debug = False):\n",
    "    # Create a new corpus directory\n",
    "    corpus_dir = \"./corpus/\"\n",
    "    if os.path.exists(corpus_dir):\n",
    "        for f in os.listdir(corpus_dir):\n",
    "            os.remove(os.path.join(corpus_dir, f))\n",
    "    os.makedirs(corpus_dir, exist_ok=True)\n",
    "\n",
    "    # Save the uploaded files to corpus directory\n",
    "    for file in uploaded_files:\n",
    "        filename = os.path.basename(file.name)\n",
    "        src = open(file.name, \"rb\")\n",
    "        dst = open(os.path.join(corpus_dir, filename), \"wb\")\n",
    "        dst.write(src.read())\n",
    "\n",
    "    # Should take a list of strings as context\n",
    "    if debug:\n",
    "        print(f\"Detecting hallucination for question: {question} with answer: {answer}\")\n",
    "    \n",
    "    # Initialize the retriever with the corpus directory\n",
    "    doc_retriever = retriever(corpus_dir = corpus_dir, debug = debug)\n",
    "\n",
    "    # Retrieve relevant documents from the corpus\n",
    "    if debug:\n",
    "        print(f\"Retrieving relevant documents for question: {question}\")\n",
    "    retrieved_docs = doc_retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Predict hallucination using the predict_hallucination function\n",
    "    detected_hallucination = detect_hallucinations(\n",
    "        context = retrieved_docs,\n",
    "        question = question,\n",
    "        answer = answer,\n",
    "        debug = debug\n",
    "    )\n",
    "\n",
    "    # Check if an hallucination was detected\n",
    "    if detected_hallucination:\n",
    "        hallucination_was_found = \"Hallucinations detected\"\n",
    "    else:\n",
    "        hallucination_was_found = \"No hallucinations found\"\n",
    "\n",
    "    # Create an output string based on the result\n",
    "    hallucination_str = \"\"\n",
    "    for hallucination in detected_hallucination:\n",
    "        hallucination_str += f\"\"\"\n",
    "           '{hallucination['text']}' - Confidence = {hallucination['confidence']}\\n\n",
    "        \"\"\"\n",
    "\n",
    "    return hallucination_was_found, hallucination_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fcc583",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_application = gr.Interface(\n",
    "    fn = gradio_backend,\n",
    "    allow_flagging = \"never\",\n",
    "    inputs = [\n",
    "        # Drag and drop files, returns a list of file paths\n",
    "        gr.File(label = \"Upload PDF/txt files\",\n",
    "                file_count = 'multiple',\n",
    "                file_types = ['.pdf', '.txt']),\n",
    "        gr.Textbox(label = \"Prompt\",\n",
    "                   placeholder = \"Type your question here...\"),\n",
    "        gr.Textbox(label = \"Answer\",\n",
    "                   lines = 3,\n",
    "                   placeholder = \"type the answer here...\"),\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Textbox(label = \"Status\"),\n",
    "        gr.Textbox(label = \"Detected Hallucinations\",\n",
    "                   lines = 5)\n",
    "    ],\n",
    "    title = \"RAG system with Hallucination Detection\",\n",
    "    description = \"\"\"\n",
    "        Upload a PDF document and ask any question.\n",
    "        The chatbot will try to answer using the provided document.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "rag_application.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3817f49c",
   "metadata": {},
   "source": [
    "# 6. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26972858",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "**Question:** Which car model features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\n",
    "\n",
    "**Answer (correct):** The Audi A4 2024 features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges.\n",
    "\n",
    "**Status:** Hallucinations detected\n",
    "\n",
    "**Detected hallucinations:** 'The Audi A4 2024' - Confidence = 0.9719486832618713\n",
    "\n",
    "We retrieve the relevant documents for the query in order to understand this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d2eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_retriever = retriever(corpus_dir = \"./example_inputs/\", debug = True)\n",
    "docs = test_retriever.get_relevant_documents(\"Which car model features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4f7abd",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "LettuceDetect is a framework that detects hallucinations based on the abscence of data given a certain context that is assumed to be true, rather than on the actual validity of the evaluated answer.\n",
    "\n",
    "Thus, when implemented in combination with a RAG system it should be used as a pre-processing step in order to point out possible conflicts with a given corpus of data, without assuming them to be necessarly true. In the previous example, the name of the car is correct. However, it is not shown in the relevant chunks (as these are about the requested information about the digital instrument display) and therefore is pointed out as an hallucination.\n",
    "\n",
    "Later on, these conflicts can be evaluated by LLMs to determine their factual accuracy and provide an enhanced response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
