{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7885d414",
   "metadata": {},
   "source": [
    "# 1. Description of the project\n",
    "\n",
    "In this project, a RAG system is implemented and used in combination with LettuceDetect.\n",
    "\n",
    "# 2. Setup\n",
    "\n",
    "1. **Install these packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0477ba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                                  Version\n",
      "---------------------------------------- -----------\n",
      "accelerate                               1.9.0\n",
      "aiofiles                                 24.1.0\n",
      "aiohappyeyeballs                         2.6.1\n",
      "aiohttp                                  3.12.15\n",
      "aiosignal                                1.4.0\n",
      "annotated-types                          0.7.0\n",
      "antlr4-python3-runtime                   4.9.3\n",
      "anyio                                    4.10.0\n",
      "asttokens                                3.0.0\n",
      "attrs                                    25.3.0\n",
      "backoff                                  2.2.1\n",
      "bcrypt                                   4.3.0\n",
      "beautifulsoup4                           4.13.4\n",
      "Brotli                                   1.1.0\n",
      "build                                    1.3.0\n",
      "cachetools                               5.5.2\n",
      "certifi                                  2025.8.3\n",
      "cffi                                     1.17.1\n",
      "charset-normalizer                       3.4.2\n",
      "chromadb                                 1.0.15\n",
      "click                                    8.2.1\n",
      "coloredlogs                              15.0.1\n",
      "comm                                     0.2.3\n",
      "contourpy                                1.3.3\n",
      "cryptography                             45.0.6\n",
      "cycler                                   0.12.1\n",
      "dataclasses-json                         0.6.7\n",
      "debugpy                                  1.8.15\n",
      "decorator                                5.2.1\n",
      "Deprecated                               1.2.18\n",
      "distro                                   1.9.0\n",
      "dotenv                                   0.9.9\n",
      "durationpy                               0.10\n",
      "effdet                                   0.4.1\n",
      "emoji                                    2.14.1\n",
      "executing                                2.2.0\n",
      "fastapi                                  0.116.1\n",
      "ffmpy                                    0.6.1\n",
      "filelock                                 3.18.0\n",
      "filetype                                 1.2.0\n",
      "flatbuffers                              25.2.10\n",
      "fonttools                                4.59.0\n",
      "frozenlist                               1.7.0\n",
      "fsspec                                   2025.7.0\n",
      "google-api-core                          2.25.1\n",
      "google-auth                              2.40.3\n",
      "google-cloud-vision                      3.10.2\n",
      "googleapis-common-protos                 1.70.0\n",
      "gradio                                   5.41.0\n",
      "gradio_client                            1.11.0\n",
      "greenlet                                 3.2.3\n",
      "groovy                                   0.1.2\n",
      "grpcio                                   1.74.0\n",
      "grpcio-status                            1.74.0\n",
      "h11                                      0.16.0\n",
      "hf-xet                                   1.1.7\n",
      "html5lib                                 1.1\n",
      "httpcore                                 1.0.9\n",
      "httptools                                0.6.4\n",
      "httpx                                    0.28.1\n",
      "httpx-sse                                0.4.1\n",
      "huggingface-hub                          0.34.3\n",
      "humanfriendly                            10.0\n",
      "idna                                     3.10\n",
      "importlib_metadata                       8.7.0\n",
      "importlib_resources                      6.5.2\n",
      "ipykernel                                6.30.1\n",
      "ipython                                  9.4.0\n",
      "ipython_pygments_lexers                  1.1.1\n",
      "jedi                                     0.19.2\n",
      "Jinja2                                   3.1.6\n",
      "jiter                                    0.10.0\n",
      "joblib                                   1.5.1\n",
      "jsonpatch                                1.33\n",
      "jsonpointer                              3.0.0\n",
      "jsonschema                               4.25.0\n",
      "jsonschema-specifications                2025.4.1\n",
      "jupyter_client                           8.6.3\n",
      "jupyter_core                             5.8.1\n",
      "kiwisolver                               1.4.8\n",
      "kubernetes                               33.1.0\n",
      "langchain                                0.3.27\n",
      "langchain-chroma                         0.2.5\n",
      "langchain-community                      0.3.27\n",
      "langchain-core                           0.3.72\n",
      "langchain-openai                         0.3.9\n",
      "langchain-text-splitters                 0.3.9\n",
      "langchain-unstructured                   0.1.6\n",
      "langdetect                               1.0.9\n",
      "langsmith                                0.4.12\n",
      "lettucedetect                            0.1.7\n",
      "lxml                                     6.0.0\n",
      "markdown-it-py                           3.0.0\n",
      "MarkupSafe                               3.0.2\n",
      "marshmallow                              3.26.1\n",
      "matplotlib                               3.10.5\n",
      "matplotlib-inline                        0.1.7\n",
      "mdurl                                    0.1.2\n",
      "mmh3                                     5.2.0\n",
      "mpmath                                   1.3.0\n",
      "multidict                                6.6.3\n",
      "mypy_extensions                          1.1.0\n",
      "nest-asyncio                             1.6.0\n",
      "networkx                                 3.5\n",
      "nltk                                     3.9.1\n",
      "numpy                                    2.2.6\n",
      "nvidia-cublas-cu12                       12.6.4.1\n",
      "nvidia-cuda-cupti-cu12                   12.6.80\n",
      "nvidia-cuda-nvrtc-cu12                   12.6.77\n",
      "nvidia-cuda-runtime-cu12                 12.6.77\n",
      "nvidia-cudnn-cu12                        9.5.1.17\n",
      "nvidia-cufft-cu12                        11.3.0.4\n",
      "nvidia-cufile-cu12                       1.11.1.6\n",
      "nvidia-curand-cu12                       10.3.7.77\n",
      "nvidia-cusolver-cu12                     11.7.1.2\n",
      "nvidia-cusparse-cu12                     12.5.4.2\n",
      "nvidia-cusparselt-cu12                   0.6.3\n",
      "nvidia-nccl-cu12                         2.26.2\n",
      "nvidia-nvjitlink-cu12                    12.6.85\n",
      "nvidia-nvtx-cu12                         12.6.77\n",
      "oauthlib                                 3.3.1\n",
      "olefile                                  0.47\n",
      "omegaconf                                2.3.0\n",
      "onnx                                     1.18.0\n",
      "onnxruntime                              1.19.2\n",
      "openai                                   1.66.3\n",
      "opencv-python                            4.12.0.88\n",
      "opentelemetry-api                        1.36.0\n",
      "opentelemetry-exporter-otlp-proto-common 1.36.0\n",
      "opentelemetry-exporter-otlp-proto-grpc   1.36.0\n",
      "opentelemetry-proto                      1.36.0\n",
      "opentelemetry-sdk                        1.36.0\n",
      "opentelemetry-semantic-conventions       0.57b0\n",
      "orjson                                   3.11.1\n",
      "overrides                                7.7.0\n",
      "packaging                                25.0\n",
      "pandas                                   2.3.1\n",
      "parso                                    0.8.4\n",
      "pdf2image                                1.17.0\n",
      "pdfminer.six                             20250506\n",
      "pexpect                                  4.9.0\n",
      "pi_heif                                  1.1.0\n",
      "pikepdf                                  9.10.2\n",
      "pillow                                   11.3.0\n",
      "pip                                      24.0\n",
      "platformdirs                             4.3.8\n",
      "posthog                                  5.4.0\n",
      "prompt_toolkit                           3.0.51\n",
      "propcache                                0.3.2\n",
      "proto-plus                               1.26.1\n",
      "protobuf                                 6.31.1\n",
      "psutil                                   7.0.0\n",
      "ptyprocess                               0.7.0\n",
      "pure_eval                                0.2.3\n",
      "pyasn1                                   0.6.1\n",
      "pyasn1_modules                           0.4.2\n",
      "pybase64                                 1.4.2\n",
      "pycocotools                              2.0.10\n",
      "pycparser                                2.22\n",
      "pydantic                                 2.11.7\n",
      "pydantic_core                            2.33.2\n",
      "pydantic-settings                        2.10.1\n",
      "pydub                                    0.25.1\n",
      "Pygments                                 2.19.2\n",
      "pyparsing                                3.2.3\n",
      "pypdf                                    5.9.0\n",
      "pypdfium2                                4.30.0\n",
      "PyPika                                   0.48.9\n",
      "pyproject_hooks                          1.2.0\n",
      "python-dateutil                          2.9.0.post0\n",
      "python-dotenv                            1.1.1\n",
      "python-iso639                            2025.2.18\n",
      "python-magic                             0.4.27\n",
      "python-multipart                         0.0.20\n",
      "python-oxmsg                             0.0.2\n",
      "pytz                                     2025.2\n",
      "PyYAML                                   6.0.2\n",
      "pyzmq                                    27.0.1\n",
      "RapidFuzz                                3.13.0\n",
      "referencing                              0.36.2\n",
      "regex                                    2025.7.34\n",
      "requests                                 2.32.4\n",
      "requests-oauthlib                        2.0.0\n",
      "requests-toolbelt                        1.0.0\n",
      "rich                                     14.1.0\n",
      "rpds-py                                  0.26.0\n",
      "rsa                                      4.9.1\n",
      "ruff                                     0.12.7\n",
      "safehttpx                                0.1.6\n",
      "safetensors                              0.6.1\n",
      "scikit-learn                             1.7.1\n",
      "scipy                                    1.16.1\n",
      "semantic-version                         2.10.0\n",
      "setuptools                               80.9.0\n",
      "shellingham                              1.5.4\n",
      "six                                      1.17.0\n",
      "sniffio                                  1.3.1\n",
      "soupsieve                                2.7\n",
      "SQLAlchemy                               2.0.42\n",
      "stack-data                               0.6.3\n",
      "starlette                                0.47.2\n",
      "sympy                                    1.14.0\n",
      "tenacity                                 9.1.2\n",
      "threadpoolctl                            3.6.0\n",
      "tiktoken                                 0.10.0\n",
      "timm                                     1.0.19\n",
      "tokenizers                               0.21.4\n",
      "tomlkit                                  0.13.3\n",
      "torch                                    2.7.1\n",
      "torchvision                              0.22.1\n",
      "tornado                                  6.5.1\n",
      "tqdm                                     4.67.1\n",
      "traitlets                                5.14.3\n",
      "transformers                             4.55.0\n",
      "triton                                   3.3.1\n",
      "typer                                    0.16.0\n",
      "typing_extensions                        4.14.1\n",
      "typing-inspect                           0.9.0\n",
      "typing-inspection                        0.4.1\n",
      "tzdata                                   2025.2\n",
      "unstructured                             0.18.11\n",
      "unstructured-client                      0.42.1\n",
      "unstructured-inference                   1.0.5\n",
      "unstructured.pytesseract                 0.3.15\n",
      "urllib3                                  2.5.0\n",
      "uvicorn                                  0.35.0\n",
      "uvloop                                   0.21.0\n",
      "watchfiles                               1.1.0\n",
      "wcwidth                                  0.2.13\n",
      "webencodings                             0.5.1\n",
      "websocket-client                         1.8.0\n",
      "websockets                               15.0.1\n",
      "wrapt                                    1.17.2\n",
      "yarl                                     1.20.1\n",
      "zipp                                     3.23.0\n",
      "zstandard                                0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq langchain langchain-unstructured langchain-chroma langchain-openai unstructured langchain-community unstructured[pdf] dotenv lettucedetect==0.1.7 gradio ipykernel pydantic\n",
    "%pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4b2de",
   "metadata": {},
   "source": [
    "2. **Import the necessary modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46796ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os, io, sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from lettucedetect.models.inference import HallucinationDetector\n",
    "import gradio as gr\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af561e9",
   "metadata": {},
   "source": [
    "3. **Deploy an Azure OpenAI LLM resource and embedding resource**\n",
    "\n",
    "    Use the following link: https://ai.azure.com/\n",
    "4. **Save the details to the .env file:**\n",
    "    ```bash\n",
    "    echo AZURE_OPENAI_API_KEY=\\\"your-api-key-here\\\" >> .env\n",
    "    echo AZURE_OPENAI_API_VERSION=\\\"your-version-here\\\" >> .env\n",
    "    echo AZURE_OPENAI_ENDPOINT=\\\"your-endpoint-here\\\" >> .env\n",
    "    echo GPT_MODEL=\\\"your-llm-model-here\\\" >> .env\n",
    "    echo EMBEDDINGS_MODEL_NAME=\\\"your-embeddings-model-here\\\" >> .env\n",
    "    echo EMBEDDINGS_DEPLOYMENT=\\\"your-embeddings-deployment-here\\\" >> .env\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add108f6",
   "metadata": {},
   "source": [
    "# 3. ChromaDB setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2abe90",
   "metadata": {},
   "source": [
    "## 3.1 The text splitter\n",
    "\n",
    "The text splitter divides documents into manageable chunks to optimize downstream processing and retrieval in RAG workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f04b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_splitter(data, debug = False):\n",
    "    \"\"\" Split the documents into chunks.\"\"\"\n",
    "    if debug:\n",
    "        print(\"--- In function text_splitter ---\\n\"\n",
    "              f\"Splitting {len(data)} documents into chunks...\\n\"\n",
    "        )\n",
    "\n",
    "    # Split the documents into chunks of 1000 characters\n",
    "    # with an overlap of 50 characters\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f83fd",
   "metadata": {},
   "source": [
    "## 3.2 The document loader\n",
    "\n",
    "The document loader reads and parses files from the corpus directory into structured document objects for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235c37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(corpus_dir: str = \"\", debug = False):\n",
    "    \"\"\"Load documents from the specified corpus directory.\"\"\"\n",
    "    if debug:\n",
    "        print(\"--- In function load_documents ---\")\n",
    "    loaded_docs = []\n",
    "\n",
    "    # Load all documents from the specified corpus directory\n",
    "    if corpus_dir:\n",
    "        if debug:\n",
    "            print(f\"Loading documents from corpus directory: {corpus_dir}\\n\")\n",
    "        for file in os.listdir(corpus_dir):\n",
    "            if debug:\n",
    "                print(f\"Loading {file}...\")\n",
    "            loader = UnstructuredLoader(corpus_dir + file, mode = 'single')\n",
    "            loaded_docs.extend(loader.load())\n",
    "\n",
    "    # Filter complex metadata from loaded documents\n",
    "    if debug:\n",
    "        print(\"Filtering complex metadata...\\n\")\n",
    "    filtered_docs = filter_complex_metadata(loaded_docs)\n",
    "\n",
    "    return filtered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8207d2a",
   "metadata": {},
   "source": [
    "## 3.3 The embedding client\n",
    "\n",
    "The embedding client initializes and manages Azure OpenAI embeddings for converting text into vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821f66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(debug = False):\n",
    "    \"\"\" Initialize and return an Azure OpenAI embeddings client. \"\"\"\n",
    "    load_dotenv(find_dotenv())\n",
    "    model = os.getenv('EMBEDDINGS_MODEL_NAME')\n",
    "    api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    azure_deployment = os.getenv(\"EMBEDDINGS_DEPLOYMENT\")\n",
    "\n",
    "    # Validate required environment variables\n",
    "    if not all([model, api_key, api_version, azure_endpoint, azure_deployment]):\n",
    "        raise ValueError(\n",
    "            \"Missing environment variables.\\n \\\n",
    "            Please add all the required environment variables \\\n",
    "            in the .env file:\\n \\\n",
    "            EMBEDDINGS_MODEL_NAME, AZURE_OPENAI_API_KEY, \\\n",
    "            AZURE_OPENAI_API_VERSION, AZURE_OPENAI_ENDPOINT, \\\n",
    "            EMBEDDINGS_DEPLOYMENT\"\n",
    "        )\n",
    "    \n",
    "    # Initialize and return an Azure OpenAI embeddings client\n",
    "    if debug:\n",
    "        print(\"--- In function embeddings ---\\n\"\n",
    "              f\"Initializing embeddings with model: {model}, \"\n",
    "              \"deployment: {azure_deployment}\\n\"\n",
    "        )\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        model = model,\n",
    "        api_key = api_key,\n",
    "        api_version = api_version,\n",
    "        azure_endpoint = azure_endpoint,\n",
    "        azure_deployment = azure_deployment,\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efc443",
   "metadata": {},
   "source": [
    "## 3.4 The vector database\n",
    "\n",
    "The vector database stores document embeddings for fast similarity search and retrieval. Built with Chroma, it enables efficient access to relevant document chunks in RAG workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c2d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(document_list, debug = False):\n",
    "    \"\"\"\n",
    "    Initialize a database from a given corpus of documents.\n",
    "    \"\"\"    \n",
    "    # Create a Chroma vector store from the documents\n",
    "    if debug:\n",
    "        print(\"--- In function create_database ---\\n\"\n",
    "              f\"Creating vector database\"\n",
    "              f\"with {len(document_list)} documents...\\n\"\n",
    "    )\n",
    "        \n",
    "    # Return the vector store\n",
    "    return Chroma.from_documents(documents = document_list,\n",
    "                                 embedding = embeddings(debug = debug),\n",
    "                                 persist_directory = None,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c90de",
   "metadata": {},
   "source": [
    "# 4. The Hallucination detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8ac4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hallucinations(context, question, answer, debug = False):\n",
    "    if debug:\n",
    "        print(\"-- In function detect_hallucinations ---\\n\"\n",
    "              f\"Predicting hallucination for question: {question}\\n\"\n",
    "              f\"with answer: {answer}\\n\"\n",
    "        )\n",
    "\n",
    "    # Initialize the hallucination detector with a transformer model\n",
    "    detector = HallucinationDetector(\n",
    "        method=\"transformer\",\n",
    "        model_path=\"KRLabsOrg/lettucedect-base-modernbert-en-v1\"\n",
    "    )\n",
    "\n",
    "    # Predict hallucination using the detector\n",
    "    result = detector.predict(context = context,\n",
    "                              question = question,\n",
    "                              answer = answer,\n",
    "                              output_format = \"spans\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485658c4",
   "metadata": {},
   "source": [
    "# 5. The interface\n",
    "\n",
    "In this section a GUI is created for the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e045e76c",
   "metadata": {},
   "source": [
    "## 5.1 The Corpus class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc78aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \"\"\"\n",
    "    A class to handle corpus creation and file uploads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus_dir: str = \"./corpus/\", keep_files = False,\n",
    "                 debug = False):\n",
    "        \"\"\"\n",
    "        Initialize the Corpus with a directory path.\n",
    "\n",
    "        If the directory exists, it will be emptied by default.\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            print(\"--- In Corpus constructor ---\\n\"\n",
    "                  f\"Creating corpus at {corpus_dir}...\\n\"\n",
    "            )\n",
    "        \n",
    "        # Save the directory for later\n",
    "        self.directory = corpus_dir\n",
    "\n",
    "        # Unless specified, empty the specified corpus directory\n",
    "        if not keep_files:\n",
    "            if os.path.exists(self.directory):\n",
    "                for f in os.listdir(self.directory):\n",
    "                    os.remove(os.path.join(self.directory, f))\n",
    "            os.makedirs(self.directory, exist_ok=True)\n",
    "\n",
    "    def upload_files(self, uploaded_files, debug = False):\n",
    "        \"\"\"\n",
    "        Save the uploaded files to the corpus directory\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            print(\"--- In function Retriever.upload_files ---\\n\"\n",
    "                  f\"Uploading {len(uploaded_files)} files \"\n",
    "                  \"to corpus directory...\\n\"\n",
    "            )\n",
    "\n",
    "        # Save each uploaded file to the corpus directory (if any)\n",
    "        for file in uploaded_files:\n",
    "            filename = os.path.basename(file.name)\n",
    "            src = open(file.name, \"rb\")\n",
    "            dst = open(os.path.join(self.directory, filename), \"wb\")\n",
    "            dst.write(src.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce1a063",
   "metadata": {},
   "source": [
    "## 5.2 The Retriever class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e461cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    \"\"\"\n",
    "    A retriever class to handle document retrieva from a given corpus directory.\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus: Corpus, debug = False):\n",
    "        \"\"\"\n",
    "        Initialize the retriever with a corpus directory\n",
    "        \"\"\"\n",
    "\n",
    "        if debug:\n",
    "            print(\"--- In Retriever constructor ---\\n\"\n",
    "                  f\"Creating retriever from {corpus.directory}...\\n\"\n",
    "            )\n",
    "\n",
    "        # Save the corpus directory\n",
    "        self.corpus = corpus\n",
    "\n",
    "        # Using instance variables so the destructor is not called\n",
    "        self.docs = load_documents(self.corpus.directory, debug = debug)\n",
    "        self.chunks = text_splitter(self.docs, debug = debug)\n",
    "        self.vectordb = create_database(self.chunks, debug = debug)\n",
    "        self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "    def get_full_docs(self, doc_list: list[Document],\n",
    "                      debug = False) -> list[Document]:\n",
    "        \"\"\" Load full documents from the specified document list. \"\"\"\n",
    "\n",
    "        if debug:\n",
    "            print(\"--- In function get_full_docs ---\\n\"\n",
    "                f\"Loading full documents from {len(doc_list)} filepaths...\\n\"\n",
    "            )\n",
    "\n",
    "        # Extract the sources from the document metadata\n",
    "        sources = {doc.metadata.get('source')\n",
    "            for doc in doc_list\n",
    "            if 'source' in doc.metadata\n",
    "        }\n",
    "\n",
    "        # For each path, add the file content to the list of full documents\n",
    "        full_docs = []\n",
    "        for path in sources:\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                full_document = Document(page_content = file.read())\n",
    "                full_docs.append(full_document)\n",
    "            if debug:\n",
    "                print(f\"Loaded document: {path}\")\n",
    "        return full_docs\n",
    "    \n",
    "    def deep_search(self, answer: str, retrieved_docs: list[Document],\n",
    "                    debug = False) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Retrieve additional context documents relevant to the answer\n",
    "        In the documents retrieved for the question\n",
    "        \"\"\"\n",
    "\n",
    "        if debug:\n",
    "            print(\"--- In function Retriever.deep_search ---\\n\"\n",
    "                  f\"Retrieving documents relevant to the answer: {answer}\\n\"\n",
    "                  \"From the documents retrieved for the question...\\n\"\n",
    "            )\n",
    "\n",
    "        # Obtain the sources from the retrieved documents\n",
    "        sources = {doc.metadata.get('source')\n",
    "                   for doc in retrieved_docs\n",
    "                   if 'source' in doc.metadata\n",
    "        }\n",
    "\n",
    "        # Obtain the relevant documents for the answer\n",
    "        retrieved_docs_answer = self.retriever.get_relevant_documents(answer)\n",
    "\n",
    "        # Filter the retrieved documents to only include those\n",
    "        # that are in the sources of the retrieved documents\n",
    "        retrieved_docs_answer = [\n",
    "            doc for doc in retrieved_docs_answer\n",
    "            if doc.metadata.get('source') in sources\n",
    "        ]\n",
    "\n",
    "        # Join the retrieved documents with the original ones\n",
    "        retrieved_docs.extend(retrieved_docs_answer)\n",
    "        if debug:\n",
    "            print(f\"Number of documents retrieved in deep search: \"\n",
    "                  f\"{len(retrieved_docs_answer)}\\n\"\n",
    "                  f\"Retrieved documents: {[doc.metadata.get('source', 'Unknown')\n",
    "                                           for doc in retrieved_docs_answer]}\\n\"\n",
    "            )\n",
    "        \n",
    "        # Return the retrieved documents\n",
    "        return retrieved_docs\n",
    "\n",
    "    def get_relevant_documents(self, question: str = \"\", answer: str = \"\", \n",
    "                               full_docs=False, include_answer=False,\n",
    "                               deep_search=False, debug=False) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents from the corpus\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve documents relevant to the question\n",
    "        if debug:\n",
    "            print(\"--- In function Retriever.get_relevant_documents ---\\n\"\n",
    "                  \"Retrieving relevant documents for question:\\n\"\n",
    "                  f\"{question}\\n\"\n",
    "            )\n",
    "        retrieved_docs = self.retriever.get_relevant_documents(question)\n",
    "        if debug:\n",
    "            print(\"Retrieving relevant documents for question...\\n\"\n",
    "                  f\"Number of retrieved documents: {len(retrieved_docs)}\\n\"\n",
    "                  f\"Retrieved documents: {[doc.metadata.get('source', 'Unknown')\n",
    "                                           for doc in retrieved_docs]}\\n\"\n",
    "            )\n",
    "            \n",
    "        # If deep search is enabled, retrieve documents relevant to the answer\n",
    "        # in the documents retrieved for the question\n",
    "        if deep_search:\n",
    "            if not include_answer:\n",
    "                raise ValueError(\n",
    "                    \"Deep search is enabled, but include_answer is False. \"\n",
    "                    \"Please set include_answer to True to use deep search.\"\n",
    "                )\n",
    "            retrieved_docs = self.deep_search(\n",
    "                answer = answer,\n",
    "                retrieved_docs = retrieved_docs,\n",
    "                debug = debug\n",
    "            )\n",
    "        \n",
    "        # If deep search is disabled and include_answer is True,\n",
    "        # retrieve documents relevant to the answer\n",
    "        elif include_answer:\n",
    "            retrieved_docs_answer = self.retriever.get_relevant_documents(answer)\n",
    "            retrieved_docs = retrieved_docs + retrieved_docs_answer\n",
    "\n",
    "        # If full_docs mode is enabled, return the complete retrieved documents\n",
    "        if full_docs:\n",
    "            retrieved_docs = self.get_full_docs(retrieved_docs, debug = debug)\n",
    "\n",
    "        # Print the retrieved documents if debug mode is enabled\n",
    "        if debug:\n",
    "            print(f\"Final number of documents: {len(retrieved_docs)}\\n\"\n",
    "                  f\"Retrieved documents: {[doc.metadata.get('source', 'Unknown')\n",
    "                                           for doc in retrieved_docs]}\\n\"\n",
    "            )\n",
    "\n",
    "        # Return the retrieved documents\n",
    "        return retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9223fd43",
   "metadata": {},
   "source": [
    "## 5.3 The LLM checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bd576b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMVerifier:\n",
    "    \"\"\"\n",
    "    A class to handle LLM-based hallucination checker\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, retriever: Retriever, debug = False):\n",
    "        \"\"\"\n",
    "        Initialize the LLMVerifier with a retriever instance.\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            print(\"--- In LLMVerifier constructor ---\\n\"\n",
    "                  \"Creating LLMVerifier with environment variables...\\n\"\n",
    "            )\n",
    "        self.debug = debug\n",
    "\n",
    "        # Save the retriever instance\n",
    "        self.retriever = retriever\n",
    "\n",
    "        # Get environment variables\n",
    "        load_dotenv(find_dotenv())\n",
    "        api_version = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "        azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "        api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "        gpt_model = os.getenv('GPT_MODEL')\n",
    "        \n",
    "        # Validate required environment variables\n",
    "        if not all([api_version, azure_endpoint, api_key, gpt_model]):\n",
    "            raise ValueError(\n",
    "                \"\"\"\n",
    "                Missing environment variables.\n",
    "                Please load all the required environment variables in the .env file:\n",
    "                AZURE_OPENAI_API_VERSION, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, GPT_MODEL\n",
    "                \"\"\"\n",
    "            )\n",
    "            \n",
    "        # Initialize and return an Langchain Azure OpenAI client\n",
    "        # And prepare it for returning structured outputs\n",
    "        self.client = AzureChatOpenAI(\n",
    "            azure_deployment = gpt_model,\n",
    "            api_version=api_version,\n",
    "            azure_endpoint=azure_endpoint,\n",
    "            api_key=api_key,\n",
    "        )\n",
    "        self.structured_llm = self.client.with_structured_output(self.OutputSchema)\n",
    "\n",
    "        # Create the verification prompt template for the LLM\n",
    "        self.prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\n",
    "                \"system\",\n",
    "                (\n",
    "                    \"Your task is to evaluate statements generated by an LLM in order to determine if they are correct or not. \"\n",
    "                    \"For that, you are going to be given a certain context which is true. \"\n",
    "                    \"Later you will be given a question made by a human and an answer generated by an LLM. \"\n",
    "                    \"Your task is to determine if a certain passage of the answer is true or false based on the context. \"\n",
    "                    \"For that, first you will reason step by step on the context and the question, and derive a conclusion about the truthfulness of the statement. \"\n",
    "                    \"Finally, you will justifiy the reason behind your decision. \"\n",
    "                    \"Gidelines for the evaluation: \"\n",
    "                    \"- First, identify the position of the hallucination in the answer. \"\n",
    "                    \"- Second, identify the role of the hallucination in the context. \"\n",
    "                    \"- Next, reason about the correctness of the hallucinated part given the question, answer and context. Does it answer the question in a logically correct way? \"\n",
    "                    \"- If the content is correct given the context, return True. \"\n",
    "                    \"- If the input is corrupted, empty or malformed, or if it consists of a single character, return True. \"\n",
    "                    \"- If the content is incorrect, return False. For the content to be considered incorrect, it must be wrong or inconsisted based on the context. \"             \n",
    "                    \"- If the content is hallucinated, return False. For the content to be considered hallucinated, it must not be present in the context, or combine different parts of the context in a way that is not logically correct. \"\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"user\",\n",
    "                (\n",
    "                    \"Use the following context as true information: {context} \"\n",
    "                    \"Given the following question: {question}, you will evaluate a part of the following ai-generated answer: {answer} \"\n",
    "                    \"The part of the answer to evaluate is: {hallucination}.\"\n",
    "                ),\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    class OutputSchema(BaseModel):\n",
    "        \"\"\"\n",
    "        Output schema for the LLMVerifier.\n",
    "        \"\"\"\n",
    "        reasoning_question: str = Field(..., description = \"Identify what is being asked in the question, and what does the user want to know.\")\n",
    "        reasoning_answer: str = Field(..., description = \"Identify the position of the hallucination in the answer, as it's a part of it.\")\n",
    "        reasoning_context: str = Field(..., description = \"Identify the relation of the hallucination with the context, and how it relates to the question and answer.\")\n",
    "        reasoning_answer_type_match: str = Field(..., description = \"Determine whether the type of the answer matches the type of information requested in the question.\")\n",
    "        reasoning_correctness: str = Field(..., description = \"Reason about the correctness of the hallucinated part given the question, answer and context. Does it answer the question in a logically correct way?\")\n",
    "        is_correct_first_thought: bool = Field(..., description = \"Using the guidelines, decide if the hallucination is correct or not.\")\n",
    "        justification: str = Field(..., description = \"Justification for the decision, based on the reasoning and guidelines.\")\n",
    "        is_correct: bool = Field(..., description = \"Whether the hallucination is correct or not after reasoning, having a first thought and justifying the decision.\")\n",
    "\n",
    "    def is_correct(self, context: list[Document], question: str,\n",
    "                         answer: str, hallucination: str):\n",
    "        \"\"\"\n",
    "        Check the validity of the hallucination given a context using the LLM.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert the context documents to a string\n",
    "        # by joining the page content of each document\n",
    "        context_str = set(doc.page_content for doc in context)\n",
    "        if self.debug:\n",
    "            print(\"--- In function LLMVerifier.verify_hallucination ---\\n\"\n",
    "                  f\"Verifying hallucination: {hallucination}\\n\"\n",
    "                  f\"Using context: {context_str}\\n\"\n",
    "                  f\"With question: {question}\\n\"\n",
    "                  f\"With answer: {answer}\\n\"\n",
    "            )\n",
    "\n",
    "        # Obtain the response from the LLM using the prompt template\n",
    "        chain = self.prompt_template | self.structured_llm\n",
    "        response = chain.invoke({\n",
    "            \"context\": \"\\n\".join(context_str),\n",
    "            \"answer\": answer,\n",
    "            \"question\": question,\n",
    "            \"hallucination\": hallucination\n",
    "        })\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"LLM Response:\")\n",
    "            print(f\"Reasoning on the question: {response.reasoning_question}\")\n",
    "            print(f\"Reasoning on the answer: {response.reasoning_answer}\")\n",
    "            print(f\"Reasoning on the context: {response.reasoning_context}\")\n",
    "            print(f\"Reasoning on the answer type match: {response.reasoning_answer_type_match}\")\n",
    "            print(f\"Reasoning on the correctness: {response.reasoning_correctness}\")\n",
    "            print(f\"Is correct (first thought): {response.is_correct_first_thought}\")\n",
    "            print(f\"Justification: {response.justification}\")\n",
    "            print(f\"Is correct: {response.is_correct}\")\n",
    "        \n",
    "        # Return whether the hallucination is detected or not\n",
    "        return response.is_correct\n",
    "\n",
    "    def verify_hallucinations(self, detected_hallucinations: list[dict],\n",
    "                              question: str, answer: str) -> str:\n",
    "        \"\"\"\n",
    "        Verify a list of detected hallucinations using the LLM.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(\"--- In function LLMVerifier.verify_hallucinations ---\\n\"\n",
    "                  f\"Verifying {len(detected_hallucinations)} hallucinations \"\n",
    "                  f\"for question: {question}\\n\"\n",
    "                  f\"And answer: {answer}\\n\"\n",
    "            )\n",
    "\n",
    "        # Retrieve the relevant documents for the question\n",
    "        retrieved_docs_question = self.retriever.get_relevant_documents(question)\n",
    "        retrieved_docs_answer = self.retriever.get_relevant_documents(answer)\n",
    "        retrieved_docs = retrieved_docs_question + retrieved_docs_answer\n",
    "\n",
    "        # Verify each hallucination using the LLM\n",
    "        final_hallucinations = \"\"\n",
    "        for hallucination in detected_hallucinations:\n",
    "            # Verify each hallucination using the LLM\n",
    "            is_correct = self.is_correct(\n",
    "                context = retrieved_docs,\n",
    "                question = question,\n",
    "                answer = answer,\n",
    "                hallucination = hallucination['text'],\n",
    "            )\n",
    "\n",
    "            # If the hallucination is verified, add it to the final string\n",
    "            if not is_correct:\n",
    "                final_hallucinations += (\n",
    "                    f\"\\'{hallucination['text']}\\'\"\n",
    "                    f\" - Confidence = {hallucination['confidence']}\\n\"\n",
    "                )\n",
    "\n",
    "        # Return the final hallucinations string\n",
    "        return final_hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e193675a",
   "metadata": {},
   "source": [
    "## 5.4 The backend function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00515a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_backend(uploaded_files = [], context = \"\", question = \"\", answer = \"\",\n",
    "                   full_docs = False, include_answer = False,\n",
    "                   deep_search = False, llm_verification = False, debug = False):\n",
    "    \"\"\"\n",
    "    Gradio backend function to handle file uploads and hallucination detection.\n",
    "    \"\"\"\n",
    "\n",
    "    # If debug mode is enabled, redirect the standard output to a string\n",
    "    if debug:\n",
    "        debug_output_stream = io.StringIO()\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = debug_output_stream\n",
    "        \n",
    "    # Initialize variables for the output\n",
    "    print(\"--- In function gradio_backend ---\")\n",
    "    hallucination_was_found = \"No hallucinations found\"\n",
    "    hallucination_str = \"\"\n",
    "    error_output = gr.update(\n",
    "        value = \"\",\n",
    "        visible = False\n",
    "    )\n",
    "    llm_output = gr.update(\n",
    "        value = \"\",\n",
    "        visible = False\n",
    "    )\n",
    "    debug_output = gr.update(\n",
    "        value = \"\",\n",
    "        visible = False\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Initialize the corpus and save the uploaded files and context\n",
    "        corpus = Corpus(corpus_dir = \"./temp/\", debug = debug)\n",
    "        if uploaded_files:\n",
    "            corpus.upload_files(uploaded_files, debug = debug)\n",
    "        if context:\n",
    "            with open(\"./temp/context.txt\", \"w\") as f:\n",
    "                f.write(context)\n",
    "        if not uploaded_files and not context:\n",
    "            raise ValueError(\n",
    "                \"No files or context provided. \"\n",
    "                \"Please upload files or provide context.\"\n",
    "            )\n",
    "            \n",
    "\n",
    "        # Initialize the retriever with the corpus and retrieve relevant documents\n",
    "        retriever = Retriever(corpus = corpus, debug = debug)\n",
    "        retrieved_docs = retriever.get_relevant_documents(\n",
    "            question = question,\n",
    "            answer = answer,\n",
    "            full_docs = full_docs,\n",
    "            include_answer = include_answer,\n",
    "            deep_search = deep_search,\n",
    "            debug = debug\n",
    "        )\n",
    "\n",
    "        # Predict hallucination using the predict_hallucination function\n",
    "        detected_hallucinations = detect_hallucinations(\n",
    "            context = retrieved_docs,\n",
    "            question = question,\n",
    "            answer = answer,\n",
    "            debug = debug\n",
    "        )\n",
    "\n",
    "        # Check if an hallucination was detected\n",
    "        if detected_hallucinations:\n",
    "            hallucination_was_found = \"Hallucinations detected\"\n",
    "\n",
    "        # Create an output string based on the result\n",
    "        for hallucination in detected_hallucinations:\n",
    "            hallucination_str += (\n",
    "                f\"\\'{hallucination['text']}\\'\"\n",
    "                f\" - Confidence = {hallucination['confidence']}\\n\"\n",
    "            )\n",
    "\n",
    "        # If LLM verification is enabled, verify the hallucinations using the LLM\n",
    "        if llm_verification:\n",
    "            # Initialize the LLM verifier\n",
    "            llm_verifier = LLMVerifier(retriever = retriever, debug = debug)\n",
    "            \n",
    "            # Verify the hallucinations using the LLM\n",
    "            final_hallucinations = llm_verifier.verify_hallucinations(\n",
    "                detected_hallucinations = detected_hallucinations,\n",
    "                question = question,\n",
    "                answer = answer\n",
    "            )\n",
    "                    \n",
    "            # Initialize the llm output with the verified hallucinations\n",
    "            llm_output = gr.update(\n",
    "                value = final_hallucinations,\n",
    "                visible = True\n",
    "            )\n",
    "\n",
    "    except Exception as exc:\n",
    "        error_output = gr.update(\n",
    "            value = str(exc),\n",
    "            visible = True\n",
    "        )\n",
    "\n",
    "    finally:\n",
    "        if debug:\n",
    "            # Restore the original standard output\n",
    "            sys.stdout = original_stdout\n",
    "\n",
    "            # Store the debug output\n",
    "            debug_output = gr.update(\n",
    "                value = debug_output_stream.getvalue(),\n",
    "                visible = True\n",
    "            )\n",
    "\n",
    "    return hallucination_was_found, hallucination_str, error_output, llm_output, debug_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4922e7a",
   "metadata": {},
   "source": [
    "## 5.5 The interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69fcc583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/gradio/interface.py:425: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_application = gr.Interface(\n",
    "    fn = gradio_backend,\n",
    "    allow_flagging = \"never\",\n",
    "    inputs = [\n",
    "        # Drag and drop files, returns a list of file paths\n",
    "        gr.File(\n",
    "            label = \"Upload PDF/txt files\",\n",
    "            file_count = 'multiple',\n",
    "            file_types = ['.pdf', '.txt']\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label = \"Context\",\n",
    "            placeholder = \"Provide additional context here...\"\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label = \"Prompt\",\n",
    "            placeholder = \"Type your question here...\"\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label = \"Answer\",\n",
    "            lines = 3,\n",
    "            placeholder = \"type the answer here...\"\n",
    "        ),\n",
    "        gr.Checkbox(\n",
    "            label = \"Full documents mode\",\n",
    "            value = False\n",
    "        ),\n",
    "        gr.Checkbox(\n",
    "            label = \"Include answer in relevance search\",\n",
    "            value = False\n",
    "        ),\n",
    "        gr.Checkbox(\n",
    "            label = \"Deep search\",\n",
    "            value = False\n",
    "        ),\n",
    "        gr.Checkbox(\n",
    "            label = \"LLM verifier\",\n",
    "            value = False,\n",
    "        ),\n",
    "        gr.Checkbox(\n",
    "            label = \"Debug mode\",\n",
    "            value = False\n",
    "        ),\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Textbox(label = \"Status\"),\n",
    "        gr.Textbox(label = \"Detected Hallucinations\"),\n",
    "        gr.Textbox(label = \"Error Output\", visible = False),\n",
    "        gr.Textbox(label = \"LLM Verified Hallucinations\", visible = False),\n",
    "        gr.Textbox(label = \"Debug Output\", visible = False),\n",
    "    ],\n",
    "    title = \"RAG system with Hallucination Detection\",\n",
    "    description = \"Upload a collection of pdf or txt files provide a prompt \\\n",
    "                   and a response. The backend will try to detect \\\n",
    "                   hallucinations in the response based on the context.\"\n",
    ")\n",
    "\n",
    "rag_application.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3817f49c",
   "metadata": {},
   "source": [
    "# 6. Tests\n",
    "\n",
    "After completing the setup, several tests were carried in order to ensure the optimal performance of the system. Each one reflects a state of the project, and justifies the adjustements that were made in the code in order to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26972858",
   "metadata": {},
   "source": [
    "## Test 1\n",
    "\n",
    "This test was done before the answer was included in the input for retrieving relevant chunks of information. Thus, only the question was useful for finding the relevant chunks of information.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "**Question:** Which car model features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\n",
    "\n",
    "**Answer (correct):** The Audi A4 2024 features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges.\n",
    "\n",
    "**Full docs:** False\n",
    "\n",
    "**Include answer:** False\n",
    "\n",
    "**Deep search:** False\n",
    "\n",
    "**LLM verifier:** False\n",
    "\n",
    "### Outputs\n",
    "\n",
    "**Status:** Hallucinations detected\n",
    "\n",
    "**Detected hallucinations:** 'The Audi A4 2024' - Confidence = 0.9719486832618713\n",
    "\n",
    "We retrieve the relevant documents for the query in order to understand this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d2eb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- In Corpus constructor ---\n",
      "Creating corpus at ./example_inputs/...\n",
      "\n",
      "--- In Retriever constructor ---\n",
      "Creating retriever from ./example_inputs/...\n",
      "\n",
      "--- In function load_documents ---\n",
      "Loading documents from corpus directory: ./example_inputs/\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './example_inputs/'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m test_corpus = Corpus(corpus_dir = \u001b[33m\"\u001b[39m\u001b[33m./example_inputs/\u001b[39m\u001b[33m\"\u001b[39m, debug = \u001b[38;5;28;01mTrue\u001b[39;00m, keep_files = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m test_retriever = \u001b[43mRetriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_corpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m docs = test_retriever.get_relevant_documents(question = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m    Which car model features a 12.3-inch high-resolution digital instrument\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m    display that replaces traditional analog gauges?\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m)\n\u001b[32m      7\u001b[39m docs\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mRetriever.__init__\u001b[39m\u001b[34m(self, corpus, debug)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mself\u001b[39m.corpus = corpus\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Using instance variables so the destructor is not called\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28mself\u001b[39m.docs = \u001b[43mload_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.chunks = text_splitter(\u001b[38;5;28mself\u001b[39m.docs, debug = debug)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mself\u001b[39m.vectordb = create_database(\u001b[38;5;28mself\u001b[39m.chunks, debug = debug)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mload_documents\u001b[39m\u001b[34m(corpus_dir, debug)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading documents from corpus directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorpus_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[32m     13\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './example_inputs/'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- In function gradio_backend ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "/tmp/ipykernel_11682/457611322.py:103: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = self.retriever.get_relevant_documents(question)\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/lettucedetect/models/inference.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, device=self.device)\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/lettucedetect/models/inference.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, device=self.device)\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/lettucedetect/models/inference.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, device=self.device)\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/lettucedetect/models/inference.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, device=self.device)\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/lettucedetect/models/inference.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, device=self.device)\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "test_corpus = Corpus(corpus_dir = \"./example_inputs/\", debug = True, keep_files = True)\n",
    "test_retriever = Retriever(test_corpus, debug = True)\n",
    "docs = test_retriever.get_relevant_documents(question = \"\"\"\n",
    "    Which car model features a 12.3-inch high-resolution digital instrument\n",
    "    display that replaces traditional analog gauges?\n",
    "    \"\"\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6571d",
   "metadata": {},
   "source": [
    "The problem is that the 'Audi A4 2024' part of the answer is not found in the context that is similar to the question, even though they are part of the same document. In the following tests, we explore 2 different approaches to solving this issue:\n",
    "\n",
    "- Finding a way to provide additional context to the detector so the missing parts are not flagged as hallucinations (done through tests 2 - 5).\n",
    "\n",
    "- Processing a text passage marked as an hallucination once it has been flagged in order to find out if it's truly an hallucination (done in test 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a4637",
   "metadata": {},
   "source": [
    "## Test 2\n",
    "\n",
    "The same test as before but now including the answer in the similarity search.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "**Question:** Which car model features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\n",
    "\n",
    "**Answer (correct):** The Audi A4 2024 features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges.\n",
    "\n",
    "**Full docs:** False\n",
    "\n",
    "**Include answer:** True\n",
    "\n",
    "**Deep search:** False\n",
    "\n",
    "**LLM verifier:** False\n",
    "\n",
    "### Outputs\n",
    "\n",
    "**Status:** No hallucinations detected\n",
    "\n",
    "**Detected hallucinations:**\n",
    "\n",
    "We retrieve the relevant documents for the question and provided answer together. In this case, the relevant information relative to the \"Audi A4 2024\" model is included in the retrieved chunks of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d909b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = Corpus(corpus_dir = \"./example_inputs/\", debug = True, keep_files = True)\n",
    "test_retriever = Retriever(test_corpus, debug = True)\n",
    "docs = test_retriever.get_relevant_documents(\"Which car model features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\"\n",
    "                                             +\n",
    "                                             \"The Audi 2024 features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges.\"\n",
    ")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6a439",
   "metadata": {},
   "source": [
    "In the following code block, we join the retrievals for the question and answer separately in order to provide a greater context to the system.\n",
    "\n",
    "**Note:** In the output, 8 out of 10 documents are retrieved due to the similarity of their contents. Keep in mind that all the example inputs are AI-generated car descriptions. In a scenario where the content is more diverse, the relative amount of retrieved documents compared to the whole corpus is expected to be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a355369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = Corpus(corpus_dir = \"./example_inputs/\", debug = True, keep_files = True)\n",
    "test_retriever = Retriever(test_corpus, debug = True)\n",
    "question = \"\"\"\n",
    "    Which car model features a 12.3-inch high-resolution digital \n",
    "    instrument display that replaces traditional analog gauges?\n",
    "\"\"\"\n",
    "answer = \"\"\"\n",
    "    The Audi 2024 features a 12.3-inch high-resolution digital\n",
    "    instrument display that replaces traditional analog gauges.\n",
    "\"\"\"\n",
    "test_retriever.get_relevant_documents(question, answer, include_answer = True,\n",
    "                                      debug = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abdfdb2",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "We chose to retrieve documents for the question and answer separately, then combine them, to ensure both the context of the query and the specific entities or facts in the answer are represented. This prevents missing relevant information and reduces false positives in hallucination detection. Separate retrievals maintain clarity and provide LettuceDetect with a fuller context for accurate verification.\n",
    "\n",
    "Additionally, providing more comprehensive context reduces the risk of false negatives, as it increases the likelihood that supporting evidence for true statements will be included in the verification process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e58f8",
   "metadata": {},
   "source": [
    "## Test 3\n",
    "\n",
    "In this case, we successfully trick the model by providing a wrong response but combining actual information present in the retrieved documents.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "**Question:** Which car features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\n",
    "\n",
    "**Answer (incorrect):** The TFSI four-cylinder engine features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges\n",
    "\n",
    "**Full docs:** False\n",
    "\n",
    "**Include answer:** True\n",
    "\n",
    "**Deep search:** False\n",
    "\n",
    "**LLM verifier:** False\n",
    "\n",
    "### Outputs\n",
    "\n",
    "**Status:** No hallucinations detected\n",
    "\n",
    "**Detected hallucinations:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731496ef",
   "metadata": {},
   "source": [
    "## Test 4\n",
    "\n",
    "In this test, the functionality for loading the entire documents containing the relevant context is added.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "**Question:** Which car features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\n",
    "\n",
    "**Answer (incorrect):** The TFSI four-cylinder engine features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges\n",
    "\n",
    "**Full docs:** True\n",
    "\n",
    "**Include answer:** True\n",
    "\n",
    "**Deep search:** False\n",
    "\n",
    "**LLM verifier:** False\n",
    "\n",
    "### Outputs\n",
    "\n",
    "**Status:** No hallucinations detected\n",
    "\n",
    "**Detected hallucinations:**\n",
    "\n",
    "### Conclusion \n",
    "\n",
    "This method seemed promising but failed because BERT is not prepared to handle long inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06b72b",
   "metadata": {},
   "source": [
    "## Test 5\n",
    "\n",
    "In this test, we try the **deep search** strategy: first we find the relevant context found in the documents related to the question.\n",
    "Next, we enrich this context with context related to the answer found in the previous documents.\n",
    "This way, the information is extracted only from the relevant documents for the question, but the important information relative to the answer is not lost.\n",
    "At the same time, adding information from other contexts unrelated to the question is prevented.\n",
    "\n",
    "First, we check the retrieved context for the same question as before, this time using the deep search strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ed8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = Corpus(corpus_dir = \"./example_inputs/\", debug = True, keep_files = True)\n",
    "test_retriever = Retriever(test_corpus, debug = True)\n",
    "\n",
    "question = (\n",
    "    \"Which car model features a 12.3-inch high-resolution digital\"\n",
    "    \"instrument display that replaces traditional analog gauges?\"\n",
    ")\n",
    "answer = (\n",
    "    \"The Audi 2024 features a 12.3-inch high-resolution digital\"\n",
    "    \"instrument display that replaces traditional analog gauges.\"\n",
    ")\n",
    "          \n",
    "test_retriever.get_relevant_documents(\n",
    "    question = question, answer = answer, full_docs = False,\n",
    "    include_answer = True, deep_search = True, debug = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603cace7",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "\n",
    "**Question:** Which car features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\n",
    "\n",
    "**Answer (incorrect):** The TFSI four-cylinder engine features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges\n",
    "\n",
    "**Full docs:** False\n",
    "\n",
    "**Include answer:** True\n",
    "\n",
    "**Deep search:** True\n",
    "\n",
    "**LLM verifier:** False\n",
    "\n",
    "### Outputs\n",
    "\n",
    "**Status:** No hallucinations detected\n",
    "\n",
    "**Detected hallucinations:**\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "When using the deep search strategy the problem persists: even though all the context comes from the files relevant to the question, the detector is unable to differentiate when some information is inadecuate even though it's found in the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2e3fd",
   "metadata": {},
   "source": [
    "## Test 6\n",
    "\n",
    "For this test, we develop a functionality to check the text passages marked as hallucinations by using an LLM client. The strategy details are the following:\n",
    "\n",
    "- We've noticed that adding context similar to the answer increases the number of false negatives. Thus, we will not use the **include answer** and **deep search** settings. **Full docs** setting will also be deactivated so BERT can handle all text chunks. We will try to decrease the number of false negatives as much as possible, at the expense of an increased number of false positives. Those will be handled in the following way:\n",
    "\n",
    "- After flagging all potential hallucinations (as seen before, most of them will be false positives), we will retrieve the relevant context for both the question and answer together. As seen before, using LettuceDetect with extra context from the answer can result in an increased number of false negatives. Thus, will not use it for this purpose.\n",
    "\n",
    "- Finally, we will provide a LLM with both the relevant context, the detected hallucination and the complete answer, and ask it to check whether it's an hallucination or not.\n",
    "\n",
    "First, we will test it with the example that tricked the model with the previous strategies.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "**Question:** Which car features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\n",
    "\n",
    "**Answer (incorrect):** The TFSI four-cylinder engine features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges\n",
    "\n",
    "**Full docs:** True\n",
    "\n",
    "**Include answer:** True\n",
    "\n",
    "**Deep search:** False\n",
    "\n",
    "**LLM verifier:** False\n",
    "\n",
    "### Outputs\n",
    "\n",
    "**Status:** Hallucinations detected\n",
    "\n",
    "**Detected Hallucinations:** 'The TFSI four-cylinder engine' - Confidence = 0.9390618801116943\n",
    "\n",
    "**LLM Verified Hallucinations:** 'The TFSI four-cylinder engine' - Confidence = 0.9390618801116943"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe13bc5",
   "metadata": {},
   "source": [
    "## Test 7\n",
    "\n",
    "We continue testing the LLM-based approach, this time with a correct answer to see if the model correctly handles false positives\n",
    "\n",
    "### Inputs\n",
    "\n",
    "**Question:** Which car features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\n",
    "\n",
    "**Answer (incorrect):** The Audi A4 2024 features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges\n",
    "\n",
    "**Full docs:** True\n",
    "\n",
    "**Include answer:** True\n",
    "\n",
    "**Deep search:** False\n",
    "\n",
    "**LLM verifier:** False\n",
    "\n",
    "### Outputs\n",
    "\n",
    "**Status:** Hallucinations detected\n",
    "\n",
    "**Detected Hallucinations:** 'TFSI four-cylinder engine' - Confidence = 0.7963008284568787\n",
    "\n",
    "**LLM Verified Hallucinations:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5473a759",
   "metadata": {},
   "source": [
    "## Test 8\n",
    "\n",
    "We continue testing the LLM verifier by providing an hallucinated answer, expecting the model to correctly detect it as an hallucination.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "**Question:** Which car features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\n",
    "\n",
    "**Answer (incorrect):** The BMW 3 Series 2024 features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges\n",
    "\n",
    "**Full docs:** True\n",
    "\n",
    "**Include answer:** True\n",
    "\n",
    "**Deep search:** False\n",
    "\n",
    "**LLM verifier:** False\n",
    "\n",
    "### Outputs\n",
    "\n",
    "**Status:** Hallucinations detected\n",
    "\n",
    "**Detected Hallucinations:** 'BMW 3 Series 20' - Confidence = 0.6695796847343445\n",
    "\n",
    "**LLM Verified Hallucinations:** 'BMW 3 Series 20' - Confidence = 0.6695796847343445"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
