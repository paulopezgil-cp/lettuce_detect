{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7885d414",
   "metadata": {},
   "source": [
    "# 1. Description of the project\n",
    "\n",
    "In this project, a RAG system is implemented and used in combination with LettuceDetect.\n",
    "\n",
    "# 2. Setup\n",
    "\n",
    "1. **Install these packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qq langchain langchain-unstructured langchain-chroma langchain-openai unstructured langchain-community unstructured[pdf] dotenv lettucedetect gradio ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4b2de",
   "metadata": {},
   "source": [
    "2. **Import the necessary modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46796ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_core.documents.base import Document\n",
    "import os, io, sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from lettucedetect.models.inference import HallucinationDetector\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af561e9",
   "metadata": {},
   "source": [
    "3. **Deploy an Azure OpenAI LLM resource and embedding resource**\n",
    "\n",
    "    Use the following link: https://ai.azure.com/\n",
    "4. **Save the details to the .env file:**\n",
    "    ```bash\n",
    "    echo AZURE_OPENAI_API_KEY=\\\"your-api-key-here\\\" >> .env\n",
    "    echo AZURE_OPENAI_API_VERSION=\\\"your-version-here\\\" >> .env\n",
    "    echo AZURE_OPENAI_ENDPOINT=\\\"your-endpoint-here\\\" >> .env\n",
    "    echo GPT_MODEL=\\\"your-llm-model-here\\\" >> .env\n",
    "    echo EMBEDDINGS_MODEL_NAME=\\\"your-embeddings-model-here\\\" >> .env\n",
    "    echo EMBEDDINGS_DEPLOYMENT=\\\"your-embeddings-deployment-here\\\" >> .env\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add108f6",
   "metadata": {},
   "source": [
    "# 3. ChromaDB setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2abe90",
   "metadata": {},
   "source": [
    "## 3.1 The text splitter\n",
    "\n",
    "The text splitter divides documents into manageable chunks to optimize downstream processing and retrieval in RAG workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_splitter(data, debug = False):\n",
    "    \"\"\" Split the documents into chunks.\"\"\"\n",
    "    if debug:\n",
    "        print(\"--- In function text_splitter ---\\n\"\n",
    "              f\"Splitting {len(data)} documents into chunks...\\n\"\n",
    "        )\n",
    "\n",
    "    # Split the documents into chunks of 1000 characters\n",
    "    # with an overlap of 50 characters\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f83fd",
   "metadata": {},
   "source": [
    "## 3.2 The document loader\n",
    "\n",
    "The document loader reads and parses files from the corpus directory into structured document objects for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(corpus_dir: str = \"\", debug = False):\n",
    "    \"\"\"Load documents from the specified corpus directory.\"\"\"\n",
    "    if debug:\n",
    "        print(\"--- In function load_documents ---\")\n",
    "    loaded_docs = []\n",
    "\n",
    "    # Load all documents from the specified corpus directory\n",
    "    if corpus_dir:\n",
    "        if debug:\n",
    "            print(f\"Loading documents from corpus directory: {corpus_dir}\\n\")\n",
    "        for file in os.listdir(corpus_dir):\n",
    "            if debug:\n",
    "                print(f\"Loading {file}...\")\n",
    "            loader = UnstructuredLoader(corpus_dir + file, mode = 'single')\n",
    "            loaded_docs.extend(loader.load())\n",
    "\n",
    "    # Filter complex metadata from loaded documents\n",
    "    if debug:\n",
    "        print(\"Filtering complex metadata...\\n\")\n",
    "    filtered_docs = filter_complex_metadata(loaded_docs)\n",
    "\n",
    "    return filtered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8207d2a",
   "metadata": {},
   "source": [
    "## 3.3 The embedding client\n",
    "\n",
    "The embedding client initializes and manages Azure OpenAI embeddings for converting text into vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(debug = False):\n",
    "    \"\"\" Initialize and return an Azure OpenAI embeddings client. \"\"\"\n",
    "    load_dotenv(find_dotenv())\n",
    "    model = os.getenv('EMBEDDINGS_MODEL_NAME')\n",
    "    api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    azure_deployment = os.getenv(\"EMBEDDINGS_DEPLOYMENT\")\n",
    "\n",
    "    # Validate required environment variables\n",
    "    if not all([model, api_key, api_version, azure_endpoint, azure_deployment]):\n",
    "        raise ValueError(\n",
    "            \"Missing environment variables.\\n \\\n",
    "            Please add all the required environment variables \\\n",
    "            in the .env file:\\n \\\n",
    "            EMBEDDINGS_MODEL_NAME, AZURE_OPENAI_API_KEY, \\\n",
    "            AZURE_OPENAI_API_VERSION, AZURE_OPENAI_ENDPOINT, \\\n",
    "            EMBEDDINGS_DEPLOYMENT\"\n",
    "        )\n",
    "    \n",
    "    # Initialize and return an Azure OpenAI embeddings client\n",
    "    if debug:\n",
    "        print(\"--- In function embeddings ---\\n\"\n",
    "              f\"Initializing embeddings with model: {model}, \"\n",
    "              \"deployment: {azure_deployment}\\n\"\n",
    "        )\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        model = model,\n",
    "        api_key = api_key,\n",
    "        api_version = api_version,\n",
    "        azure_endpoint = azure_endpoint,\n",
    "        azure_deployment = azure_deployment,\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efc443",
   "metadata": {},
   "source": [
    "## 3.4 The vector database\n",
    "\n",
    "The vector database stores document embeddings for fast similarity search and retrieval. Built with Chroma, it enables efficient access to relevant document chunks in RAG workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c2d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(document_list, debug = False):\n",
    "    \"\"\"\n",
    "    Initialize a database from a given corpus of documents.\n",
    "    \"\"\"    \n",
    "    # Create a Chroma vector store from the documents\n",
    "    if debug:\n",
    "        print(\"--- In function create_database ---\\n\"\n",
    "              f\"Creating vector database\"\n",
    "              f\"with {len(document_list)} documents...\\n\"\n",
    "    )\n",
    "        \n",
    "    # Return the vector store\n",
    "    return Chroma.from_documents(documents = document_list,\n",
    "                                 embedding = embeddings(debug = debug),\n",
    "                                 persist_directory = None,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96237913",
   "metadata": {},
   "source": [
    "## 3.5 Test retriever\n",
    "\n",
    "The retriever fetches relevant document chunks from the vector database using embeddings to match user queries with semantically similar content for efficient retrieval in RAG workflows. It is not based on LLMs, but purely on a similarity search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081902dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from the specified corpus directory\n",
    "test_docs = load_documents(corpus_dir = \"./example_inputs/\", debug = True)\n",
    "test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the metadata attribute 'source' exists in the first document\n",
    "test_docs[0].metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5992ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the loaded documents into chunks\n",
    "test_chunks = text_splitter(test_docs, debug = True)\n",
    "print(f\"Number of chunks created: {len(test_chunks)}\")\n",
    "test_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff483219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector database from the chunks\n",
    "test_vectordb = create_database(test_chunks, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_retriever = test_vectordb.as_retriever()\n",
    "test_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_relevant_docs = test_retriever.get_relevant_documents(\"\"\"\n",
    "    Which car model provides exceptional ground clearance and capability?\n",
    "\"\"\")\n",
    "test_relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2554e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, doc in enumerate(test_relevant_docs):\n",
    "    print(f\"{idx}: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b381d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the complete retrieved documents\n",
    "test_sources = {doc.metadata.get('source') for doc in test_relevant_docs if 'source' in doc.metadata}\n",
    "test_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c90de",
   "metadata": {},
   "source": [
    "# 4. The Hallucination detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ac4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hallucinations(context, question, answer, debug = False):\n",
    "    if debug:\n",
    "        print(\"-- In function detect_hallucinations ---\\n\"\n",
    "              f\"Predicting hallucination for question: {question}\"\n",
    "              f\"with answer: {answer}\\n\"\n",
    "        )\n",
    "\n",
    "    # Initialize the hallucination detector with a transformer model\n",
    "    detector = HallucinationDetector(\n",
    "        method=\"transformer\",\n",
    "        model_path=\"KRLabsOrg/lettucedect-base-modernbert-en-v1\"\n",
    "    )\n",
    "\n",
    "    # Predict hallucination using the detector\n",
    "    result = detector.predict(context = context,\n",
    "                              question = question,\n",
    "                              answer = answer,\n",
    "                              output_format = \"spans\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485658c4",
   "metadata": {},
   "source": [
    "# 5. The interface\n",
    "\n",
    "In this section a GUI is created for the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e045e76c",
   "metadata": {},
   "source": [
    "## 5.1 The Corpus class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \"\"\"\n",
    "    A class to handle corpus creation and file uploads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus_dir: str = \"./corpus/\", keep_files = False,\n",
    "                 debug = False):\n",
    "        \"\"\"\n",
    "        Initialize the Corpus with a directory path.\n",
    "\n",
    "        If the directory exists, it will be emptied by default.\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            print(\"--- In Corpus constructor ---\\n\"\n",
    "                  f\"Creating corpus at {corpus_dir}...\\n\"\n",
    "            )\n",
    "        \n",
    "        # Save the directory for later\n",
    "        self.directory = corpus_dir\n",
    "\n",
    "        # Unless specified, empty the specified corpus directory\n",
    "        if not keep_files:\n",
    "            if os.path.exists(self.directory):\n",
    "                for f in os.listdir(self.directory):\n",
    "                    os.remove(os.path.join(self.directory, f))\n",
    "            os.makedirs(self.directory, exist_ok=True)\n",
    "\n",
    "    def upload_files(self, uploaded_files, debug = False):\n",
    "        \"\"\"\n",
    "        Save the uploaded files to the corpus directory\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            print(\"--- In function Retriever.upload_files ---\\n\"\n",
    "                  f\"Uploading {len(uploaded_files)} files \"\n",
    "                  \"to corpus directory...\\n\"\n",
    "            )\n",
    "\n",
    "        # Save each uploaded file to the corpus directory (if any)\n",
    "        for file in uploaded_files:\n",
    "            filename = os.path.basename(file.name)\n",
    "            src = open(file.name, \"rb\")\n",
    "            dst = open(os.path.join(self.directory, filename), \"wb\")\n",
    "            dst.write(src.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce1a063",
   "metadata": {},
   "source": [
    "## 5.2 The Retriever class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e461cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    \"\"\"\n",
    "    A retriever class to handle document retrieva from a given corpus directory.\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus: Corpus, debug = False):\n",
    "        \"\"\"\n",
    "        Initialize the retriever with a corpus directory\n",
    "        \"\"\"\n",
    "\n",
    "        if debug:\n",
    "            print(\"--- In Retriever constructor ---\\n\"\n",
    "                  f\"Creating retriever from {corpus.directory}...\\n\"\n",
    "            )\n",
    "\n",
    "        # Save the corpus directory\n",
    "        self.corpus = corpus\n",
    "\n",
    "        # Using instance variables so the destructor is not called\n",
    "        self.docs = load_documents(self.corpus.directory, debug = debug)\n",
    "        self.chunks = text_splitter(self.docs, debug = debug)\n",
    "        self.vectordb = create_database(self.chunks, debug = debug)\n",
    "        self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "    def get_full_docs(self, doc_list: list[Document],\n",
    "                      debug = False) -> list[Document]:\n",
    "        \"\"\" Load full documents from the specified document list. \"\"\"\n",
    "\n",
    "        if debug:\n",
    "            print(\"--- In function get_full_docs ---\\n\"\n",
    "                f\"Loading full documents from {len(doc_list)} filepaths...\\n\"\n",
    "            )\n",
    "\n",
    "        # Extract the sources from the document metadata\n",
    "        sources = {doc.metadata.get('source')\n",
    "            for doc in doc_list\n",
    "            if 'source' in doc.metadata\n",
    "        }\n",
    "\n",
    "        # For each path, add the file content to the list of full documents\n",
    "        full_docs = []\n",
    "        for path in sources:\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                full_document = Document(page_content = file.read())\n",
    "                full_docs.append(full_document)\n",
    "            if debug:\n",
    "                print(f\"Loaded document: {path}\")\n",
    "        return full_docs\n",
    "    \n",
    "    def deep_search(self, answer: str, retrieved_docs: list[Document],\n",
    "                    debug = False) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Retrieve additional context documents relevant to the answer\n",
    "        In the documents retrieved for the question\n",
    "        \"\"\"\n",
    "\n",
    "        if debug:\n",
    "            print(\"--- In function Retriever.deep_search ---\\n\"\n",
    "                  f\"Retrieving documents relevant to the answer: {answer}\\n\"\n",
    "                  \"From the documents retrieved for the question...\\n\"\n",
    "            )\n",
    "\n",
    "        # Obtain the sources from the retrieved documents\n",
    "        sources = {doc.metadata.get('source')\n",
    "                   for doc in retrieved_docs\n",
    "                   if 'source' in doc.metadata\n",
    "        }\n",
    "\n",
    "        # Obtain the relevant documents for the answer\n",
    "        retrieved_docs_answer = self.retriever.get_relevant_documents(answer)\n",
    "\n",
    "        # Filter the retrieved documents to only include those\n",
    "        # that are in the sources of the retrieved documents\n",
    "        retrieved_docs_answer = [\n",
    "            doc for doc in retrieved_docs_answer\n",
    "            if doc.metadata.get('source') in sources\n",
    "        ]\n",
    "\n",
    "        # Join the retrieved documents with the original ones\n",
    "        retrieved_docs.extend(retrieved_docs_answer)\n",
    "        if debug:\n",
    "            print(f\"Number of documents retrieved in deep search: \"\n",
    "                  f\"{len(retrieved_docs_answer)}\\n\"\n",
    "                  f\"Retrieved documents: {[doc.metadata.get('source', 'Unknown')\n",
    "                                           for doc in retrieved_docs_answer]}\\n\"\n",
    "            )\n",
    "        \n",
    "        # Return the retrieved documents\n",
    "        return retrieved_docs\n",
    "\n",
    "    def get_relevant_documents(self, question: str = \"\", answer: str = \"\", \n",
    "                               full_docs=False, include_answer=False,\n",
    "                               deep_search=False, debug=False) -> list[Document]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents from the corpus\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve documents relevant to the question\n",
    "        if debug:\n",
    "            print(\"--- In function Retriever.get_relevant_documents ---\\n\"\n",
    "                  \"Retrieving relevant documents for question:\\n\"\n",
    "                  f\"{question}\\n\"\n",
    "            )\n",
    "        retrieved_docs = self.retriever.get_relevant_documents(question)\n",
    "        if debug:\n",
    "            print(\"Retrieving relevant documents for question...\\n\"\n",
    "                  f\"Number of retrieved documents: {len(retrieved_docs)}\\n\"\n",
    "                  f\"Retrieved documents: {[doc.metadata.get('source', 'Unknown')\n",
    "                                           for doc in retrieved_docs]}\\n\"\n",
    "            )\n",
    "            \n",
    "        # If deep search is enabled, retrieve documents relevant to the answer\n",
    "        # in the documents retrieved for the question\n",
    "        if deep_search:\n",
    "            if not include_answer:\n",
    "                raise ValueError(\n",
    "                    \"Deep search is enabled, but include_answer is False. \"\n",
    "                    \"Please set include_answer to True to use deep search.\"\n",
    "                )\n",
    "            retrieved_docs = self.deep_search(\n",
    "                answer = answer,\n",
    "                retrieved_docs = retrieved_docs,\n",
    "                debug = debug\n",
    "            )\n",
    "        \n",
    "        # If deep search is disabled and include_answer is True,\n",
    "        # retrieve documents relevant to the answer\n",
    "        elif include_answer:\n",
    "            retrieved_docs_answer = self.retriever.get_relevant_documents(answer)\n",
    "            retrieved_docs = retrieved_docs + retrieved_docs_answer\n",
    "\n",
    "        # If full_docs mode is enabled, return the complete retrieved documents\n",
    "        if full_docs:\n",
    "            retrieved_docs = self.get_full_docs(retrieved_docs, debug = debug)\n",
    "\n",
    "        # Print the retrieved documents if debug mode is enabled\n",
    "        if debug:\n",
    "            print(f\"Final number of documents: {len(retrieved_docs)}\\n\"\n",
    "                  f\"Retrieved documents: {[doc.metadata.get('source', 'Unknown')\n",
    "                                           for doc in retrieved_docs]}\\n\"\n",
    "            )\n",
    "\n",
    "        # Return the retrieved documents\n",
    "        return retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e193675a",
   "metadata": {},
   "source": [
    "## 5.3 The backend function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00515a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_backend(uploaded_files = [], context = \"\", question = \"\", answer = \"\",\n",
    "                   full_docs = False, include_answer = False,\n",
    "                   deep_search = False, debug = False):\n",
    "    \"\"\"\n",
    "    Gradio backend function to handle file uploads and hallucination detection.\n",
    "    \"\"\"\n",
    "\n",
    "    # If debug mode is enabled, redirect the standard output to a string\n",
    "    if debug:\n",
    "        debug_output_stream = io.StringIO()\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = debug_output_stream\n",
    "        \n",
    "    # Initialize variables for the output\n",
    "    print(\"--- In function gradio_backend ---\")\n",
    "    hallucination_was_found = \"No hallucinations found\"\n",
    "    hallucination_str = \"\"\n",
    "    error_output = gr.update(\n",
    "        value = \"\",\n",
    "        visible = False\n",
    "    )\n",
    "    debug_output = gr.update(\n",
    "        value = \"\",\n",
    "        visible = False\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Initialize the corpus and save the uploaded files and context\n",
    "        corpus = Corpus(corpus_dir = \"./temp/\", debug = debug)\n",
    "        if uploaded_files:\n",
    "            corpus.upload_files(uploaded_files, debug = debug)\n",
    "        if context:\n",
    "            with open(\"./temp/context.txt\", \"w\") as f:\n",
    "                f.write(context)\n",
    "        if not uploaded_files and not context:\n",
    "            raise ValueError(\n",
    "                \"No files or context provided. \"\n",
    "                \"Please upload files or provide context.\"\n",
    "            )\n",
    "            \n",
    "\n",
    "        # Initialize the retriever with the corpus and retrieve relevant documents\n",
    "        retriever = Retriever(corpus = corpus, debug = debug)\n",
    "        retrieved_docs = retriever.get_relevant_documents(\n",
    "            question = question,\n",
    "            answer = answer,\n",
    "            full_docs = full_docs,\n",
    "            include_answer = include_answer,\n",
    "            deep_search = deep_search,\n",
    "            debug = debug\n",
    "        )\n",
    "\n",
    "        # Predict hallucination using the predict_hallucination function\n",
    "        detected_hallucination = detect_hallucinations(\n",
    "            context = retrieved_docs,\n",
    "            question = question,\n",
    "            answer = answer,\n",
    "            debug = debug\n",
    "        )\n",
    "\n",
    "        # Check if an hallucination was detected\n",
    "        if detected_hallucination:\n",
    "            hallucination_was_found = \"Hallucinations detected\"\n",
    "\n",
    "        # Create an output string based on the result\n",
    "        for hallucination in detected_hallucination:\n",
    "            hallucination_str += (\n",
    "                f\"\\'{hallucination['text']}\\'\"\n",
    "                f\" - Confidence = {hallucination['confidence']}\\n\"\n",
    "            )\n",
    "\n",
    "    except Exception as exc:\n",
    "        error_output = gr.update(\n",
    "            value = str(exc),\n",
    "            visible = True\n",
    "        )\n",
    "\n",
    "    finally:\n",
    "        if debug:\n",
    "            # Restore the original standard output\n",
    "            sys.stdout = original_stdout\n",
    "\n",
    "            # Store the debug output\n",
    "            debug_output = gr.update(\n",
    "                value = debug_output_stream.getvalue(),\n",
    "                visible = True\n",
    "            )\n",
    "\n",
    "    return hallucination_was_found, hallucination_str, error_output, debug_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4922e7a",
   "metadata": {},
   "source": [
    "## 5.4 The interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fcc583",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_application = gr.Interface(\n",
    "    fn = gradio_backend,\n",
    "    allow_flagging = \"never\",\n",
    "    inputs = [\n",
    "        # Drag and drop files, returns a list of file paths\n",
    "        gr.File(\n",
    "            label = \"Upload PDF/txt files\",\n",
    "            file_count = 'multiple',\n",
    "            file_types = ['.pdf', '.txt']\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label = \"Context\",\n",
    "            placeholder = \"Provide additional context here...\"\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label = \"Prompt\",\n",
    "            placeholder = \"Type your question here...\"\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label = \"Answer\",\n",
    "            lines = 3,\n",
    "            placeholder = \"type the answer here...\"\n",
    "        ),\n",
    "        gr.Checkbox(\n",
    "            label = \"Full documents mode\",\n",
    "            value = False\n",
    "        ),\n",
    "        gr.Checkbox(\n",
    "            label = \"Include answer in relevance search\",\n",
    "            value = False\n",
    "        ),\n",
    "        gr.Checkbox(\n",
    "            label = \"Deep search\",\n",
    "            value = False\n",
    "        ),\n",
    "        gr.Checkbox(\n",
    "            label = \"Debug mode\",\n",
    "            value = False\n",
    "        ),\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Textbox(label = \"Status\"),\n",
    "        gr.Textbox(label = \"Detected Hallucinations\"),\n",
    "        gr.Textbox(label = \"Error Output\", visible = False),\n",
    "        gr.Textbox(label = \"Debug Output\", visible = False)\n",
    "    ],\n",
    "    title = \"RAG system with Hallucination Detection\",\n",
    "    description = \"Upload a collection of pdf or txt files provide a prompt \\\n",
    "                   and a response. The backend will try to detect \\\n",
    "                   hallucinations in the response based on the context.\"\n",
    ")\n",
    "\n",
    "rag_application.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3817f49c",
   "metadata": {},
   "source": [
    "# 6. Tests\n",
    "\n",
    "After completing the setup, several tests were carried in order to ensure the optimal performance of the system. Each one reflects a state of the project, and justifies the adjustements that were made in the code in order to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26972858",
   "metadata": {},
   "source": [
    "## Test 1\n",
    "\n",
    "This test was done before the answer was included in the input for retrieving relevant chunks of information. Thus, only the question was useful for finding the relevant chunks of information.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "**Question:** Which car model features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\n",
    "\n",
    "**Answer (correct):** The Audi A4 2024 features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges.\n",
    "\n",
    "**Full docs:** False\n",
    "\n",
    "**Include answer:** False\n",
    "\n",
    "**Deep search:** False\n",
    "\n",
    "### Outputs\n",
    "\n",
    "**Status:** Hallucinations detected\n",
    "\n",
    "**Detected hallucinations:** 'The Audi A4 2024' - Confidence = 0.9719486832618713\n",
    "\n",
    "We retrieve the relevant documents for the query in order to understand this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d2eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = Corpus(corpus_dir = \"./example_inputs/\", debug = True)\n",
    "test_retriever = Retriever(test_corpus, debug = True)\n",
    "docs = test_retriever.get_relevant_documents(question = \"\"\"\n",
    "    Which car model features a 12.3-inch high-resolution digital instrument\n",
    "    display that replaces traditional analog gauges?\n",
    "    \"\"\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6571d",
   "metadata": {},
   "source": [
    "The problem is that the 'Audi A4 2024' part of the answer is not found in the provided context that is similar to the answer, even though they are part of the same document. There are 2 possible options:\n",
    "\n",
    "- Finding a way to provide additional context to the detector so the missing parts are not flagged as hallucinations (done through tests 2 - 5).\n",
    "\n",
    "- Processing a text passage marked as an hallucination once it has been flagged in order to find out if it's truly an hallucination (done in test 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a4637",
   "metadata": {},
   "source": [
    "## Test 2\n",
    "\n",
    "The same test as before but now including the answer in the similarity search.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "**Question:** Which car model features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\n",
    "\n",
    "**Answer (correct):** The Audi A4 2024 features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges.\n",
    "\n",
    "**Full docs:** False\n",
    "\n",
    "**Include answer:** True\n",
    "\n",
    "**Deep search:** False\n",
    "\n",
    "### Outputs\n",
    "\n",
    "**Status:** No hallucinations detected\n",
    "\n",
    "**Detected hallucinations:**\n",
    "\n",
    "We retrieve the relevant documents for the question and provided answer together. In this case, the relevant information relative to the \"Audi A4 2024\" model is included in the retrieved chunks of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d909b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = Corpus(corpus_dir = \"./example_inputs/\", debug = True)\n",
    "test_retriever = Retriever(test_corpus, debug = True)\n",
    "docs = test_retriever.get_relevant_documents(\"Which car model features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\"\n",
    "                                             +\n",
    "                                             \"The Audi 2024 features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges.\"\n",
    ")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6a439",
   "metadata": {},
   "source": [
    "In the following code block, we join the retrievals for the question and answer separately in order to provide a greater context to the system.\n",
    "\n",
    "**Note:** In the output, 8 out of 10 documents are retrieved due to the similarity of their contents. Keep in mind that all the example inputs are AI-generated car descriptions. In a scenario where the content is more diverse, the relative amount of retrieved documents compared to the whole corpus is expected to be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a355369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = Corpus(corpus_dir = \"./example_inputs/\", debug = True)\n",
    "test_retriever = Retriever(test_corpus, debug = True)\n",
    "question = \"\"\"\n",
    "    Which car model features a 12.3-inch high-resolution digital \n",
    "    instrument display that replaces traditional analog gauges?\n",
    "\"\"\"\n",
    "answer = \"\"\"\n",
    "    The Audi 2024 features a 12.3-inch high-resolution digital\n",
    "    instrument display that replaces traditional analog gauges.\n",
    "\"\"\"\n",
    "test_retriever.get_relevant_documents(question, answer, include_answer = True,\n",
    "                                      debug = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abdfdb2",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "We chose to retrieve documents for the question and answer separately, then combine them, to ensure both the context of the query and the specific entities or facts in the answer are represented. This prevents missing relevant information and reduces false positives in hallucination detection. Separate retrievals maintain clarity and provide LettuceDetect with a fuller context for accurate verification.\n",
    "\n",
    "Additionally, providing more comprehensive context reduces the risk of false negatives, as it increases the likelihood that supporting evidence for true statements will be included in the verification process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e58f8",
   "metadata": {},
   "source": [
    "## Test 3\n",
    "\n",
    "In this case, we successfully trick the model by providing a wrong response but combining actual information present in the retrieved documents.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "**Question:** Which car features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\n",
    "\n",
    "**Answer (incorrect):** The Virtual Cockpit Technology features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges\n",
    "\n",
    "**Full docs:** False\n",
    "\n",
    "**Include answer:** True\n",
    "\n",
    "**Deep search:** False\n",
    "\n",
    "### Outputs\n",
    "\n",
    "**Status:** No hallucinations detected\n",
    "\n",
    "**Detected hallucinations:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731496ef",
   "metadata": {},
   "source": [
    "## Test 4\n",
    "\n",
    "In this test, the functionality for loading the entire documents containing the relevant context is added.\n",
    "\n",
    "### Inputs\n",
    "\n",
    "**Question:** Which car features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\n",
    "\n",
    "**Answer (incorrect):** The Virtual Cockpit Technology features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges\n",
    "\n",
    "**Full docs:** True\n",
    "\n",
    "**Include answer:** True\n",
    "\n",
    "**Deep search:** False\n",
    "\n",
    "### Outputs\n",
    "\n",
    "**Status:** No hallucinations detected\n",
    "\n",
    "**Detected hallucinations:**\n",
    "\n",
    "### Conclusion \n",
    "\n",
    "This method seemed promising but failed because BERT is not prepared to handle long inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06b72b",
   "metadata": {},
   "source": [
    "## Test 5\n",
    "\n",
    "In this test, we try the **deep search** strategy: first we find the relevant context found in the documents related to the question.\n",
    "Next, we enrich this context with context related to the answer found in the previous documents.\n",
    "This way, the information is extracted only from the relevant documents for the question, but the important information relative to the answer is not lost.\n",
    "At the same time, adding information from other contexts unrelated to the question is prevented.\n",
    "\n",
    "First, we check the retrieved context for the same question as before, this time using the deep search strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ed8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = Corpus(corpus_dir = \"./example_inputs/\", debug = True)\n",
    "test_retriever = Retriever(test_corpus, debug = True)\n",
    "\n",
    "question = (\n",
    "    \"Which car model features a 12.3-inch high-resolution digital\"\n",
    "    \"instrument display that replaces traditional analog gauges?\"\n",
    ")\n",
    "answer = (\n",
    "    \"The Audi 2024 features a 12.3-inch high-resolution digital\"\n",
    "    \"instrument display that replaces traditional analog gauges.\"\n",
    ")\n",
    "          \n",
    "test_retriever.get_relevant_documents(\n",
    "    question = question, answer = answer, full_docs = False,\n",
    "    include_answer = True, deep_search = True, debug = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603cace7",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "\n",
    "**Question:** Which car features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges?\n",
    "\n",
    "**Answer (incorrect):** The Virtual Cockpit Technology features a 12.3-inch high-resolution digital instrument display that replaces traditional analog gauges\n",
    "\n",
    "**Full docs:** True\n",
    "\n",
    "**Include answer:** True\n",
    "\n",
    "**Deep search:** False\n",
    "\n",
    "### Outputs\n",
    "\n",
    "**Status:** No hallucinations detected\n",
    "\n",
    "**Detected hallucinations:**\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "When using the deep search strategy the problem persists: even though all the context comes from the files relevant to the question, the detector is unable to differentiate when some information is inadecuate even though it's found in the context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
