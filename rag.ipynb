{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7885d414",
   "metadata": {},
   "source": [
    "# 1. Description of the project\n",
    "\n",
    "In this project, a RAG system is implemented and used in combination with LettuceDetect.\n",
    "\n",
    "# 2. Setup\n",
    "\n",
    "1. **Install these packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0477ba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq langchain langchain-unstructured langchain-chroma langchain-openai unstructured langchain-community unstructured[pdf] dotenv lettucedetect gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4b2de",
   "metadata": {},
   "source": [
    "2. **Import the necessary modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46796ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from lettucedetect.models.inference import HallucinationDetector\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af561e9",
   "metadata": {},
   "source": [
    "3. **Deploy an Azure OpenAI LLM resource and embedding resource**\n",
    "\n",
    "    Use the following link: https://ai.azure.com/\n",
    "4. **Save the details to the .env file:**\n",
    "    ```bash\n",
    "    echo AZURE_OPENAI_API_KEY=\\\"your-api-key-here\\\" >> .env\n",
    "    echo AZURE_OPENAI_API_VERSION=\\\"your-version-here\\\" >> .env\n",
    "    echo AZURE_OPENAI_ENDPOINT=\\\"your-endpoint-here\\\" >> .env\n",
    "    echo GPT_MODEL=\\\"your-llm-model-here\\\" >> .env\n",
    "    echo EMBEDDINGS_MODEL_NAME=\\\"your-embeddings-model-here\\\" >> .env\n",
    "    echo EMBEDDINGS_DEPLOYMENT=\\\"your-embeddings-deployment-here\\\" >> .env\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add108f6",
   "metadata": {},
   "source": [
    "# 3. ChromaDB setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2abe90",
   "metadata": {},
   "source": [
    "## 3.1 The text splitter\n",
    "\n",
    "The text splitter divides documents into manageable chunks to optimize downstream processing and retrieval in RAG workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f04b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_splitter(data, debug = False):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "    )\n",
    "    if debug:\n",
    "        print(f\"Splitting {len(data)} documents into chunks...\")\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f83fd",
   "metadata": {},
   "source": [
    "## 3.2 The document loader\n",
    "\n",
    "The document loader reads and parses files from the corpus directory into structured document objects for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "235c37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(corpus_dir = \"./corpus/\", debug = False):\n",
    "    # Load all documents from the corpus directory\n",
    "    loaded_docs = []\n",
    "    if debug:\n",
    "        print(f\"Loading documents from {corpus_dir}...\")\n",
    "    for file in os.listdir(corpus_dir):\n",
    "        if debug:\n",
    "            print(f\"Loading {file}...\")\n",
    "        loader = UnstructuredLoader(corpus_dir + file, mode = 'single')\n",
    "        loaded_docs.extend(loader.load())\n",
    "\n",
    "    # Filter complex metadata from loaded documents\n",
    "    if debug:\n",
    "        print(\"Filtering complex metadata...\")\n",
    "    filtered_docs = filter_complex_metadata(loaded_docs)\n",
    "\n",
    "    return filtered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8207d2a",
   "metadata": {},
   "source": [
    "## 3.3 The embedding client\n",
    "\n",
    "The embedding client initializes and manages Azure OpenAI embeddings for converting text into vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "821f66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(debug = False):\n",
    "    load_dotenv(find_dotenv())\n",
    "    model = os.getenv('EMBEDDINGS_MODEL_NAME')\n",
    "    api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    azure_deployment = os.getenv(\"EMBEDDINGS_DEPLOYMENT\")\n",
    "\n",
    "    # Validate required environment variables\n",
    "    if not all([model, api_key, api_version, azure_endpoint, azure_deployment]):\n",
    "        raise ValueError(\n",
    "            \"\"\"\n",
    "            Missing environment variables.\n",
    "            Please load all the required environment variables in the .env file:\n",
    "            EMBEDDINGS_MODEL_NAME, AZURE_OPENAI_API_KEY, AZURE_OPENAI_API_VERSION,\n",
    "            AZURE_OPENAI_ENDPOINT, EMBEDDINGS_DEPLOYMENT\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    # Initialize and return an Azure OpenAI embeddings client\n",
    "    if debug:\n",
    "        print(f\"Initializing embeddings with model: {model}, deployment: {azure_deployment}\")\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        model = model,\n",
    "        api_key = api_key,\n",
    "        api_version = api_version,\n",
    "        azure_endpoint = azure_endpoint,\n",
    "        azure_deployment = azure_deployment,\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efc443",
   "metadata": {},
   "source": [
    "## 3.4 The vector database\n",
    "\n",
    "The vector database stores document embeddings for fast similarity search and retrieval. Built with Chroma, it enables efficient access to relevant document chunks in RAG workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87c2d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(document_list, database_dir = \"./chroma_db\", debug = False):\n",
    "    # Initialize the database from a given corpus of documents\n",
    "    embedding_model = embeddings(debug = debug)\n",
    "\n",
    "    # Create the database if it doesn't exist\n",
    "    if not os.path.exists(database_dir) or not os.listdir(database_dir):\n",
    "        if debug:\n",
    "            print(f\"Creating vector database with {len(document_list)} documents...\")\n",
    "        return Chroma.from_documents(documents = document_list,\n",
    "                                                embedding = embedding_model,\n",
    "                                                persist_directory = database_dir)\n",
    "\n",
    "    # If the database exists, return it\n",
    "    return Chroma(persist_directory = database_dir,\n",
    "                  embedding_function = embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b860d2e",
   "metadata": {},
   "source": [
    "## 3.5 The retriever\n",
    "\n",
    "The retriever fetches relevant document chunks from the vector database using embeddings to match user queries with semantically similar content for efficient retrieval in RAG workflows. It is not based on LLMs, but purely on a similarity search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01c3e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(corpus_dir = \"./corpus/\", debug = False):\n",
    "    docs = load_documents(corpus_dir, debug = debug)\n",
    "    chunks = text_splitter(docs, debug = debug)\n",
    "    vectordb = create_database(chunks, debug = debug)\n",
    "    if debug:\n",
    "        print(\"Creating retriever from vector database...\")\n",
    "    retriever = vectordb.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96237913",
   "metadata": {},
   "source": [
    "## 3.6 Small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5992ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from ./example_inputs/...\n",
      "Loading ford_f150_lightning_2024.txt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audi_a4_2024.txt...\n",
      "Loading mercedes_cclass_2024.txt...\n",
      "Loading tesla_model_s_2024.txt...\n",
      "Loading toyota_camry_2024.txt...\n",
      "Loading bmw_3series_2024.txt...\n",
      "Loading jeep_wrangler_2024.txt...\n",
      "Loading porsche_911_carrera_2024.txt...\n",
      "Loading subaru_outback_2024.txt...\n",
      "Loading honda_accord_2024.txt...\n",
      "Filtering complex metadata...\n",
      "Splitting 271 documents into chunks...\n",
      "Initializing embeddings with model: text-embedding-3-small, deployment: text-embedding-3-small\n",
      "Creating retriever from vector database...\n"
     ]
    }
   ],
   "source": [
    "test_retriever = retriever(corpus_dir = \"./example_inputs/\", debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da88a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='44844552-cab2-44c9-9cf8-13945719c9dc', metadata={'filename': 'honda_accord_2024.txt', 'filetype': 'text/plain', 'source': './corpus/honda_accord_2024.txt', 'element_id': '56c4f36868129724874376f4e43bb081', 'last_modified': '2025-07-24T15:17:12', 'file_directory': './corpus', 'parent_id': '0749961253165664e3d63731c5667eee', 'category': 'NarrativeText'}, page_content='For those requiring additional power, the available 2.0-liter turbocharged four-cylinder engine generates an impressive 252 horsepower and 273 lb-ft of torque. This engine features a sophisticated cooling system and advanced turbocharger technology that delivers smooth, linear power delivery throughout the rev range.'),\n",
       " Document(id='9b1b85c5-c69d-47bd-88aa-e32d00dc0836', metadata={'file_directory': './corpus', 'filetype': 'text/plain', 'parent_id': '2259b4a98394c3141eb5a2b1cfd2d020', 'filename': 'bmw_3series_2024.txt', 'last_modified': '2025-07-24T15:17:12', 'category': 'NarrativeText', 'element_id': '3e5a795b7027cb4fab281a50049f4461', 'source': './corpus/bmw_3series_2024.txt'}, page_content='For enhanced performance, the M340i variant features a turbocharged 3.0-liter inline-six engine generating 382 horsepower and 369 lb-ft of torque. This powerplant enables acceleration from 0-60 mph in just 4.2 seconds while maintaining the refined character expected from a luxury sedan.'),\n",
       " Document(id='3488114e-1e9c-4b44-9988-a2a927d693b8', metadata={'parent_id': 'b9969aeb058e1fa36bfef9b1c44ddd1c', 'file_directory': './corpus', 'filetype': 'text/plain', 'category': 'NarrativeText', 'source': './corpus/jeep_wrangler_2024.txt', 'element_id': 'a2ea8213e5c1bbcc1a6a85a24ff73e30', 'filename': 'jeep_wrangler_2024.txt', 'last_modified': '2025-07-24T15:17:12'}, page_content='For enhanced efficiency without sacrificing capability, the available 2.0-liter turbocharged four-cylinder engine generates 270 horsepower and 295 lb-ft of torque while incorporating mild-hybrid technology with a 48-volt system that provides additional torque assistance and improved fuel economy.'),\n",
       " Document(id='59c4a444-94cb-4eea-bdff-32bced253a45', metadata={'element_id': '2030b5c46edaf188a670a45d5d3ea99c', 'file_directory': './corpus', 'last_modified': '2025-07-24T15:17:12', 'filetype': 'text/plain', 'filename': 'subaru_outback_2024.txt', 'category': 'NarrativeText', 'parent_id': '70b2e634c576d45f33bdd0a4229dcfe1', 'source': './corpus/subaru_outback_2024.txt'}, page_content='For customers requiring additional performance, the available 2.4-liter turbocharged boxer engine generates 260 horsepower and 277 lb-ft of torque. This powerplant incorporates advanced technologies including direct injection, dual active valve control system, and a sophisticated cooling system that ensures consistent performance under all operating conditions.')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = test_retriever.get_relevant_documents(\"Which car features a turbocharged 3.0-liter inline-six engine\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c90de",
   "metadata": {},
   "source": [
    "# 4. The Hallucination detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aa8ac4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hallucinations(context, question, answer, debug = False):\n",
    "    if debug:\n",
    "        print(f\"Predicting hallucination for question: {question}\")\n",
    "\n",
    "    # Initialize the hallucination detector with a transformer model\n",
    "    detector = HallucinationDetector(\n",
    "        method=\"transformer\",\n",
    "        model_path=\"KRLabsOrg/lettucedect-base-modernbert-en-v1\"\n",
    "    )\n",
    "\n",
    "    # Predict hallucination using the detector\n",
    "    result = detector.predict(context = context,\n",
    "                              question = question,\n",
    "                              answer = answer,\n",
    "                              output_format = \"spans\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485658c4",
   "metadata": {},
   "source": [
    "# 5. The interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "00515a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_backend(uploaded_files, question, answer, debug = False):\n",
    "    # Create a new corpus directory\n",
    "    corpus_dir = \"./corpus/\"\n",
    "    if os.path.exists(corpus_dir):\n",
    "        for f in os.listdir(corpus_dir):\n",
    "            os.remove(os.path.join(corpus_dir, f))\n",
    "    os.makedirs(corpus_dir, exist_ok=True)\n",
    "\n",
    "    # Save the uploaded files to corpus directory\n",
    "    for file in uploaded_files:\n",
    "        filename = os.path.basename(file.name)\n",
    "        src = open(file.name, \"rb\")\n",
    "        dst = open(os.path.join(corpus_dir, filename), \"wb\")\n",
    "        dst.write(src.read())\n",
    "\n",
    "    # Should take a list of strings as context\n",
    "    if debug:\n",
    "        print(f\"Detecting hallucination for question: {question} with answer: {answer}\")\n",
    "    \n",
    "    # Initialize the retriever with the corpus directory\n",
    "    doc_retriever = retriever(corpus_dir = corpus_dir, debug = debug)\n",
    "\n",
    "    # Retrieve relevant documents from the corpus\n",
    "    if debug:\n",
    "        print(f\"Retrieving relevant documents for question: {question}\")\n",
    "    retrieved_docs = doc_retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Predict hallucination using the predict_hallucination function\n",
    "    detected_hallucination = detect_hallucinations(\n",
    "        context = retrieved_docs,\n",
    "        question = question,\n",
    "        answer = answer,\n",
    "        debug = debug\n",
    "    )\n",
    "\n",
    "    # Check if an hallucination was detected\n",
    "    if detected_hallucination:\n",
    "        hallucination_was_found = \"Hallucinations detected\"\n",
    "    else:\n",
    "        hallucination_was_found = \"No hallucinations found\"\n",
    "\n",
    "    # Create an output string based on the result\n",
    "    hallucination_str = \"\"\n",
    "    for hallucination in detected_hallucination:\n",
    "        hallucination_str += f\"\"\"\n",
    "           '{hallucination['text']}' - Confidence = {hallucination['confidence']}\\n\n",
    "        \"\"\"\n",
    "\n",
    "    return hallucination_was_found, hallucination_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fcc583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/gradio/interface.py:425: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
      "  warnings.warn(\n",
      "INFO: HTTP Request: GET http://127.0.0.1:7864/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: HEAD http://127.0.0.1:7864/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/lettucedetect/models/inference.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, device=self.device)\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/lettucedetect/models/inference.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, device=self.device)\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/lettucedetect/models/inference.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, device=self.device)\n",
      "INFO: HTTP Request: POST https://paulo-mcw0r95x-eastus2.cognitiveservices.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "/home/pau/python/lettuce_detect/.venv/lib/python3.12/site-packages/lettucedetect/models/inference.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, device=self.device)\n"
     ]
    }
   ],
   "source": [
    "rag_application = gr.Interface(\n",
    "    fn = gradio_backend,\n",
    "    allow_flagging = \"never\",\n",
    "    inputs = [\n",
    "        # Drag and drop files, returns a list of file paths\n",
    "        gr.File(label = \"Upload PDF/txt files\",\n",
    "                file_count = 'multiple',\n",
    "                file_types = ['.pdf', '.txt']),\n",
    "        gr.Textbox(label = \"Prompt\",\n",
    "                   placeholder = \"Type your question here...\"),\n",
    "        gr.Textbox(label = \"Answer\",\n",
    "                   lines = 3,\n",
    "                   placeholder = \"type the answer here...\"),\n",
    "    ],\n",
    "    outputs = [\n",
    "        gr.Textbox(label = \"Status\"),\n",
    "        gr.Textbox(label = \"Detected Hallucinations\",\n",
    "                   lines = 5)\n",
    "    ],\n",
    "    title = \"RAG system with Hallucination Detection\",\n",
    "    description = \"\"\"\n",
    "        Upload a PDF document and ask any question.\n",
    "        The chatbot will try to answer using the provided document.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "rag_application.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
