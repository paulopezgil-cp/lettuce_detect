{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474d23b7",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "LettuceDetect is a new lightweight, open-source framework designed to identify hallucinations in Retrieval-Augmented Generation (RAG) systems.\n",
    "\n",
    "This framework addresses two critical limitations of existing methods by utilizing ModernBERT's extended context capabilities and being significantly more computationally efficient than comparable Large Language Model (LLM)-based approaches:\n",
    "- Context window constraints of traditional encoder-based methods\n",
    "\n",
    "- Computational inefficiency of LLM-based approaches\n",
    "\n",
    "Evaluated on the RAGTruth benchmark dataset, LettuceDetect outperforms previous encoder-based models and most prompt-based models in detecting unsupported claims at a token level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a93a38e",
   "metadata": {},
   "source": [
    "# 2. HallucinationDetector class\n",
    "\n",
    "I've documented the class HallucinationDetector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ffb735",
   "metadata": {},
   "source": [
    "## 2.1 The constructor\n",
    "\n",
    "`def __init__(self, method: str = \"transformer\", **kwargs)`\n",
    "\n",
    "The constructor of the HallucinationDetector class accepts an argument called method of type string. This argument can take the following values:\n",
    "\n",
    "- `\"transformer\"`: In this case, the constructor delegates the task to the class TransformerDetector\n",
    "\n",
    "- `\"llm\"`: In this case, the constructor delegates the task to the class \"LLMDetector\"\n",
    "\n",
    "### 2.1.1 The TransformerDetector class\n",
    "```\n",
    "def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        max_length: int = 4096,\n",
    "        device=None,\n",
    "        lang: Lang = \"en\",\n",
    "        **tok_kwargs\n",
    "):\n",
    "```\n",
    "\n",
    "- `model_path`: Path to the pre-trained model. The full list of pretrained BERT transformers is available [here](https://huggingface.co/KRLabsOrg).\n",
    "- `max_length`: Maximum length of the input sequence.\n",
    "- `device`: Device to use for inference. If `None`, by default will be `torch.device(\"cuda\")` or `torch.device(\"cpu\")`\n",
    "- `lang`: Language of the model. Supported languages:\n",
    "    ```\n",
    "    \"en\": \"English\",\n",
    "    \"de\": \"German\",\n",
    "    \"fr\": \"French\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"it\": \"Italian\",\n",
    "    \"pl\": \"Polish\",\n",
    "    \"cn\": \"Chinese\",\n",
    "    ```\n",
    "- `tok_kwargs`: Additional keyword arguments for the tokenizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d01b73",
   "metadata": {},
   "source": [
    "### 2.1.2 The LLMDetector class\n",
    "```\n",
    "def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-4.1-mini\",\n",
    "        temperature: float = 0.0,\n",
    "        lang: Lang = \"en\",\n",
    "        zero_shot: bool = False,\n",
    "        fewshot_path: str | None = None,\n",
    "        prompt_path: str | None = None,\n",
    "        cache_file: str | None = None,\n",
    "):\n",
    "```\n",
    "\n",
    "- `model`: The model to use for hallucination detection.\n",
    "- `temperature`: The temperature to use for hallucination detection.\n",
    "- `lang`: The language to use for hallucination detection (cf. the previous language list).\n",
    "- `zero_shot`: Whether to use zero-shot hallucination detection.\n",
    "- `fewshot_path`: The path to the few-shot examples.\n",
    "- `prompt_path`: The path to the prompt.\n",
    "- `cache_file`: The path to the cache file.\n",
    "\n",
    "When using the LLMDetector, only the 'spans' output format is supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e85c3",
   "metadata": {},
   "source": [
    "## 2.2 The predict function\n",
    "\n",
    "```\n",
    "def predict(\n",
    "    self,\n",
    "    context: list[str],\n",
    "    answer: str,\n",
    "    question: str | None = None,\n",
    "    output_format: str = \"tokens\",\n",
    ") -> list:\n",
    "```\n",
    "\n",
    "This function predicts hallucination tokens or spans given passages and an answer.\n",
    "\n",
    "- context: List of passages that were supplied to the LLM / user.\n",
    "- answer: Model‑generated answer to inspect.\n",
    "- question: Original question (``None`` for summarisation).\n",
    "- output_format: ``\"tokens\"`` for token‑level dicts, ``\"spans\"`` for character spans.\n",
    "\n",
    "It first uses `PromptUtils.format_context()` to format the context and question intro a single prompt and then it uses the internal `_predict()` function to convert a single (prompt, answer) pair into hallucination spans.\n",
    "\n",
    "### 2.2.1 Comments on the predict function of the LLMDetector class\n",
    "\n",
    "Two important things to note for this case:\n",
    "\n",
    "1. The specified output format can only be `\"spans\"`. Otherwise, an exception will be thrown.\n",
    "\n",
    "2. This function is written so it uses the OpenAI API.\n",
    "\n",
    "3. This is the prompt that they use to generate the answers:\n",
    "\n",
    "```\n",
    "<task>\n",
    "You are an expert annotator who must identify hallucinated substrings in a generated **answer** with respect to a given **source**.\n",
    "\n",
    "## Language\n",
    "- The source and answer are written in **${lang}**.\n",
    "- Respond **in ${lang} only**.\n",
    "\n",
    "## Step‑by‑step instructions\n",
    "1. **Read** the answer inside <answer>…</answer>.\n",
    "2. **Compare** each statement with the information in <source>…</source>.\n",
    "   - *Hallucination* = a substring that **(a)** contradicts the source **or** **(b)** introduces facts not supported by the source.\n",
    "   - *Not hallucination* = a substring that is consistent with the source.\n",
    "   - *Boileplate substring* = a substring that is not a hallucination but is not relevant to the question (e.g. introductory phrases, etc.)\n",
    "3. **Decide** whether the answer contains any hallucinations, be precise, in your answer only include substrings that are hallucinations.\n",
    "4. **Return** a JSON object following *exactly* this schema  \n",
    "   (no extra keys, no markdown, no code‑block fences):\n",
    "\n",
    "   `{\"hallucination_list\": [\"substring1\", \"substring2\", …]}`\n",
    "\n",
    "   If none are found, return `{\"hallucination_list\": []}`.\n",
    "</task>\n",
    "\n",
    "${fewshot_block}\n",
    "\n",
    "<source>\n",
    "${context}\n",
    "</source>\n",
    "\n",
    "<answer>\n",
    "${answer}\n",
    "</answer>\n",
    "```\n",
    "\n",
    "### 2.2.2 The output format\n",
    "For the span format:\n",
    "```\n",
    "[{\n",
    "    'text': str,        # The hallucinated text\n",
    "    'start': int,       # Start position in answer\n",
    "    'end': int,         # End position in answer\n",
    "    'confidence': float # Model's confidence (0-1)\n",
    "}]\n",
    "```\n",
    "\n",
    "For the token format (only available for the TransformerDetector class):\n",
    "```\n",
    "[{\n",
    "    'token': str,       # The token\n",
    "    'pred': int,        # 0: supported, 1: hallucinated\n",
    "    'prob': float       # Model's confidence (0-1)\n",
    "}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1127ac4",
   "metadata": {},
   "source": [
    "# 3. Demos\n",
    "\n",
    "The following demo is extracted from the original GitHub repository of the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lettucedetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ec443",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [\n",
    "    \"France is a country in Europe. The capital of France is Paris. The population of France is 67 million.\",\n",
    "]\n",
    "question = \"What is the capital of France? What is the population of France?\"\n",
    "answer = \"The capital of France is Paris. The population of France is 67 million.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1a40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lettucedetect.models.inference import HallucinationDetector\n",
    "\n",
    "# Transformer-based detector\n",
    "detector = HallucinationDetector(\n",
    "    method=\"transformer\", model_path=\"KRLabsOrg/lettucedect-base-modernbert-en-v1\"\n",
    ")\n",
    "\n",
    "# Then predict the same way\n",
    "predictions = detector.predict(context=contexts, question=question, answer=answer, output_format=\"spans\")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d2898",
   "metadata": {},
   "source": [
    "# 4. What's next\n",
    "\n",
    "Possible ideas for using this framework:\n",
    "\n",
    "- Try to modify the prediction function so it uses a different LLM API (like Azure OpenAI).\n",
    "\n",
    "- Research how to use the detector in combination with a RAG system that uses a vector database (like ChromaDB or FAISS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4772ddc0",
   "metadata": {},
   "source": [
    "# 5. References\n",
    "\n",
    "Paper - [LettuceDetect: A Hallucination Detection Framework for RAG Applications](https://arxiv.org/abs/2502.17125)\n",
    "\n",
    "GitHub repository - [LettuceDetect](https://github.com/KRLabsOrg/LettuceDetect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
